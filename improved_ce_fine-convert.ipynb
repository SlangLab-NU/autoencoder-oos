{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693267d-9a81-4282-b9bb-d853102347c5",
   "metadata": {},
   "source": [
    "# Load CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:06.596136455Z",
     "start_time": "2023-12-01T05:47:06.586541155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "# Extracting data\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']\n",
    "\n",
    "oos_train_data = data['oos_train']\n",
    "oos_val_data = data['oos_val']\n",
    "oos_test_data = data['oos_test']\n",
    "\n",
    "# Get sentences and labels\n",
    "train_sentences = [item[0] for item in train_data]\n",
    "train_labels = [item[1] for item in train_data]\n",
    "\n",
    "val_sentences = [item[0] for item in val_data]\n",
    "val_labels = [item[1] for item in val_data]\n",
    "\n",
    "test_sentences = [item[0] for item in test_data]\n",
    "test_labels = [item[1] for item in test_data]\n",
    "\n",
    "oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "model_name = \"improved_ce_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54766b2c-cd88-4636-bff0-ba15c4272f11",
   "metadata": {},
   "source": [
    "# Load SLURP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73b50-057e-4813-8606-bed9789c138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_path):\n",
    "#     sentences = []\n",
    "#     scenarios = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             sentence = data.get('sentence', None)\n",
    "#             scenario = data.get('scenario', None)\n",
    "#             if sentence is not None and scenario is not None:\n",
    "#                 sentences.append(sentence)\n",
    "#                 scenarios.append(scenario)\n",
    "#     return sentences, scenarios\n",
    "\n",
    "# # Randomly select one domain to be out of scope\n",
    "# unique_scenarios = {'alarm', 'audio', 'calendar', 'cooking', 'datetime', 'email', 'general', 'iot', 'lists', 'music', 'news', 'play', 'qa', 'recommendation', 'social', 'takeaway', 'transport', 'weather'}\n",
    "# oos_scenario = random.choice(list(unique_scenarios))\n",
    "\n",
    "# # Load data from files\n",
    "# train_sentences, train_labels = load_data('slurp/dataset/slurp/train.jsonl')\n",
    "# val_sentences, val_labels = load_data('slurp/dataset/slurp/devel.jsonl')\n",
    "# test_sentences, test_labels = load_data('slurp/dataset/slurp/test.jsonl')\n",
    "\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "# model_name = \"improved_ce_model_bert_SLURP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c8984-2fc0-4dd1-bbaa-4d1b459090aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "\n",
    "        # Forward pass through the classifier layer\n",
    "        logits = self.classifier(sentence_embedding)\n",
    "        \n",
    "        return logits, sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d902b4-6d3c-4963-bfa5-9377cdda0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(embeddings):\n",
    "    n, k = embeddings.size()  # n is the batch size, k is the embedding dimension\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Calculate the mean embedding for each sample, excluding the sample itself\n",
    "    for i in range(n):\n",
    "        # Use indexing to exclude the current sample, then calculate the mean of the remaining samples\n",
    "        indices = [j for j in range(n) if j != i]\n",
    "        mean_embedding = embeddings[indices].mean(dim=0)\n",
    "        \n",
    "        # Calculate the squared Euclidean distance for the current sample\n",
    "        distance = (embeddings[i] - mean_embedding).pow(2).sum()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        loss += distance\n",
    "    \n",
    "    # Average the loss over all samples and divide by the dimension k\n",
    "    loss = loss / (n * k)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10710f00-bedb-4060-9cfc-d3a2814b766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    writer = SummaryWriter()\n",
    "    seed_value=42\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    lr = trial.suggest_categorical('lr', [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3])\n",
    "    num_epochs = 25\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "    # ed_loss_importance = trial.suggest_float('ed_loss_importance', 0.05, 0.2)\n",
    "    ed_loss_importance = 0.1\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # Model setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "    model = TextClassifier(transformer_model, len(unique_intents))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            \n",
    "            total_loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += total_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_ed_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "                ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "                ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "                total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "                total_val_loss += total_loss.item()\n",
    "                total_ce_loss += ce_loss.item()\n",
    "                total_ed_loss += ed_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(val_dataloader)\n",
    "        avg_ed_loss = total_ed_loss / len(val_dataloader)\n",
    "        writer.add_scalar(\"Validation/Average CE Loss\", avg_ce_loss, epoch)\n",
    "        writer.add_scalar(\"Validation/Average ED Loss\", avg_ed_loss, epoch)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            #torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "    trial.set_user_attr(\"training_losses\", training_losses)\n",
    "    trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "    # fine_transformer = model.transformer\n",
    "    # train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "    # val_embeddings = encode_sentences(fine_transformer, val_sentences)\n",
    "    # oos_val_embeddings = encode_sentences(fine_transformer, oos_val_sentences)\n",
    "\n",
    "    # intent_means = {}\n",
    "    # for encoded_label in np.unique(encoded_train_labels):\n",
    "    #     # Find indices where the encoded label matches\n",
    "    #     indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "    #     # Calculate the mean embedding for the current intent\n",
    "    #     intent_embeddings = train_embeddings[indices]\n",
    "    #     intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "    #     # Use the encoded label as the dictionary key\n",
    "    #     intent_means[encoded_label] = intent_mean\n",
    "    # covariance = np.cov(train_embeddings, rowvar=False)\n",
    "    # cov_inverse = inv(covariance)\n",
    "    # val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "    # oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "    # # True binary labels: 0 for in-domain and 1 for OOD\n",
    "    # y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "    # # Combine the scores\n",
    "    # y_scores = val_scores + oos_val_scores\n",
    "\n",
    "    # # Compute AUPR\n",
    "    # aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    # return aupr\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize',  study_name='improved_ce_loss_CLINC150_min_loss_feb15', storage='sqlite:///desperate.db', load_if_exists= True)\n",
    "study.optimize(objective, n_trials=1000)  # n_trials is the number of iterations\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599cd4c-86f0-44ba-ab9f-f5e7d88cb180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
