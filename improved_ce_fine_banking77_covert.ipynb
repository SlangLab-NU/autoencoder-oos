{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, average_precision_score, roc_curve\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n",
      "File \u001b[0;32m/nlu/projects/tianyi_zhang/miniconda3/envs/ood_cuda12/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54766b2c-cd88-4636-bff0-ba15c4272f11",
   "metadata": {},
   "source": [
    "# Load BANKING77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a73b50-057e-4813-8606-bed9789c138c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uuid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m oos_val_sentences \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moos_val\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     40\u001b[0m oos_test_sentences \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moos_test\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 41\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43muuid\u001b[49m\u001b[38;5;241m.\u001b[39muuid4())\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimproved_ce_model_bert_BANKING77.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uuid' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the IntentExample class and load_intent_examples function as provided\n",
    "class IntentExample:\n",
    "    def __init__(self, text, label, do_lower_case):\n",
    "        self.original_text = text\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        if do_lower_case:\n",
    "            self.text = self.text.lower()\n",
    "\n",
    "def load_intent_examples(file_path, do_lower_case=True):\n",
    "    examples = []\n",
    "    with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "        for text, label in zip(f_text, f_label):\n",
    "            e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "            examples.append(e)\n",
    "    return examples\n",
    "\n",
    "# Define paths to the dataset directories\n",
    "base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "paths = {\n",
    "    'train': f'{base_dir}/train',\n",
    "    'valid': f'{base_dir}/valid',\n",
    "    'test': f'{base_dir}/test',\n",
    "    'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "    'oos_test': f'{base_dir}/ood-oos/test'\n",
    "}\n",
    "datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# Extract sentences and labels from the loaded datasets\n",
    "train_sentences = [e.text for e in datasets['train']]\n",
    "train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "val_sentences = [e.text for e in datasets['valid']]\n",
    "val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "test_sentences = [e.text for e in datasets['test']]\n",
    "test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "model_name = str(uuid.uuid4())+\"improved_ce_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8c8984-2fc0-4dd1-bbaa-4d1b459090aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 98\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "\n",
    "        # Forward pass through the classifier layer\n",
    "        logits = self.classifier(sentence_embedding)\n",
    "        \n",
    "        return logits, sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d902b4-6d3c-4963-bfa5-9377cdda0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(embeddings):\n",
    "    n, k = embeddings.size()  # n is the batch size, k is the embedding dimension\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Calculate the mean embedding for each sample, excluding the sample itself\n",
    "    for i in range(n):\n",
    "        # Use indexing to exclude the current sample, then calculate the mean of the remaining samples\n",
    "        indices = [j for j in range(n) if j != i]\n",
    "        mean_embedding = embeddings[indices].mean(dim=0)\n",
    "        \n",
    "        # Calculate the squared Euclidean distance for the current sample\n",
    "        distance = (embeddings[i] - mean_embedding).pow(2).sum()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        loss += distance\n",
    "    \n",
    "    # Average the loss over all samples and divide by the dimension k\n",
    "    loss = loss / (n * k)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de068b6-9e61-45ce-9eff-0c607a651903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    writer = SummaryWriter()\n",
    "    seed_value=42\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    lr = trial.suggest_categorical('lr', [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3])\n",
    "    num_epochs = 25\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "    ed_loss_importance = trial.suggest_float('ed_loss_importance', 0.05, 0.3, step = 0.02)\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # Model setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "    model = TextClassifier(transformer_model, len(unique_intents))\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"cuda not available\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            \n",
    "            total_loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += total_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_ed_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "                ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "                ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "                total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "                total_val_loss += total_loss.item()\n",
    "                total_ce_loss += ce_loss.item()\n",
    "                total_ed_loss += ed_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(val_dataloader)\n",
    "        avg_ed_loss = total_ed_loss / len(val_dataloader)\n",
    "        writer.add_scalar(\"Validation/Average CE Loss\", avg_ce_loss, epoch)\n",
    "        writer.add_scalar(\"Validation/Average ED Loss\", avg_ed_loss, epoch)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "    trial.set_user_attr(\"training_losses\", training_losses)\n",
    "    trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "    writer.close()\n",
    "    fine_model = torch.load(model_name)\n",
    "    fine_model.eval()  # Put the model in evaluation mode\n",
    "    fine_model = fine_model.to(device)\n",
    "    fine_transformer = fine_model.transformer\n",
    "    train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "    val_embeddings = encode_sentences(fine_transformer, val_sentences)\n",
    "    oos_val_embeddings = encode_sentences(fine_transformer, oos_val_sentences)\n",
    "\n",
    "    intent_means = {}\n",
    "    for encoded_label in np.unique(encoded_train_labels):\n",
    "        # Find indices where the encoded label matches\n",
    "        indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "        # Calculate the mean embedding for the current intent\n",
    "        intent_embeddings = train_embeddings[indices]\n",
    "        intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "        # Use the encoded label as the dictionary key\n",
    "        intent_means[encoded_label] = intent_mean\n",
    "    covariance = np.cov(train_embeddings, rowvar=False)\n",
    "    cov_inverse = inv(covariance)\n",
    "    val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "    oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "    # True binary labels: 0 for in-domain and 1 for OOD\n",
    "    y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "    # Combine the scores\n",
    "    y_scores = val_scores + oos_val_scores\n",
    "\n",
    "    # Compute AUPR\n",
    "    aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    return aupr\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize',  study_name='improved_ce_loss_BANKING77_search', storage='sqlite:///try.db', load_if_exists= True)\n",
    "study.optimize(objective, n_trials=150)  # n_trials is the number of iterations\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599cd4c-86f0-44ba-ab9f-f5e7d88cb180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
