{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62497da5-37c0-4b12-ac47-8663ac3f5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=883070\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fc2bd-dd71-40f5-a49d-8a5cdcf5e444",
   "metadata": {},
   "source": [
    "# MTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d55b16-f4d0-4d36-b6fc-8b516d9459bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to read and parse the MTOP file\n",
    "# def read_mtop_file(file_path):\n",
    "#     data = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         for line in file:\n",
    "#             fields = line.strip().split('\\t')\n",
    "#             if len(fields) < 8:\n",
    "#                 continue  # Skip any malformed lines\n",
    "#             record = {\n",
    "#                 'ID': fields[0],\n",
    "#                 'Intent': fields[1],\n",
    "#                 'Utterance': fields[3],\n",
    "#                 'Domain': fields[4]\n",
    "#             }\n",
    "#             data.append(record)\n",
    "#     return data\n",
    "    \n",
    "# # Function to randomly select specified number of OOD domains\n",
    "# def select_ood_domains(domains, num_ood):\n",
    "#     return random.sample(sorted(list(domains)), num_ood)\n",
    "\n",
    "# # Configuration parameters\n",
    "# english_dir = 'mtop/en'\n",
    "# num_ood_domains = 2  # Number of domains to treat as OOD\n",
    "\n",
    "# # Read dataset\n",
    "# test_data = read_mtop_file(f'{english_dir}/test.txt')\n",
    "# train_data = read_mtop_file(f'{english_dir}/train.txt')\n",
    "# eval_data = read_mtop_file(f'{english_dir}/eval.txt')\n",
    "\n",
    "# # Gather all domains and select OOD domains\n",
    "# domains = set([record['Domain'] for record in train_data + test_data + eval_data])\n",
    "# ood_domains = select_ood_domains(domains, num_ood_domains)\n",
    "\n",
    "# # Initialize data structures for sentences and labels\n",
    "# train_sentences, train_labels = [], []\n",
    "# val_sentences, val_labels = [], []\n",
    "# test_sentences, test_labels = [], []\n",
    "# oos_val_sentences, oos_test_sentences = [], []\n",
    "\n",
    "# # Distribute data based on OOD selection\n",
    "# for dataset in [(train_data, train_sentences, train_labels), (eval_data, val_sentences, val_labels), (test_data, test_sentences, test_labels)]:\n",
    "#     for record in dataset[0]:\n",
    "#         if record['Domain'] in ood_domains:\n",
    "#             if dataset is eval_data:\n",
    "#                 oos_val_sentences.append(record['Utterance'])\n",
    "#             else:\n",
    "#                 oos_test_sentences.append(record['Utterance'])\n",
    "#         else:\n",
    "#             dataset[1].append(record['Utterance'])\n",
    "#             dataset[2].append(record['Domain'])\n",
    "\n",
    "# # Rebalance OOS data between validation and test\n",
    "# combined_oos = oos_val_sentences + oos_test_sentences\n",
    "# random.shuffle(combined_oos)\n",
    "# half_index = len(combined_oos) // 2\n",
    "# oos_val_sentences = combined_oos[:half_index]\n",
    "# oos_test_sentences = combined_oos[half_index:]\n",
    "# model_name = \"ce_model_bert_mtop.pth\"\n",
    "# # Check the distribution and the selected OOD domains\n",
    "# (ood_domains, len(train_sentences), len(val_sentences), len(test_sentences), len(oos_val_sentences), len(oos_test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beab02e-8ec1-4bab-a24b-71474b1e7a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OOD Domains': ['timer'],\n",
       " 'Train Set Size': 14465,\n",
       " 'Validation Set Size': 2067,\n",
       " 'Test Set Size': 4134,\n",
       " 'OOS Validation Set Size': 491,\n",
       " 'OOS Test Set Size': 997}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_mtop_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) < 8:\n",
    "                continue  # Skip any malformed lines\n",
    "            record = {\n",
    "                'ID': fields[0],\n",
    "                'Intent': fields[1],\n",
    "                'Utterance': fields[3],\n",
    "                'Domain': fields[4]\n",
    "            }\n",
    "            data.append(record)\n",
    "    return data\n",
    "\n",
    "def select_ood_domains(domains, num_ood):\n",
    "    return [\"timer\"]  # Hardcoded OOD domains\n",
    "\n",
    "# Configuration parameters\n",
    "english_dir = 'mtop/en'\n",
    "\n",
    "# Read dataset\n",
    "all_data = read_mtop_file(f'{english_dir}/test.txt') + \\\n",
    "           read_mtop_file(f'{english_dir}/train.txt') + \\\n",
    "           read_mtop_file(f'{english_dir}/eval.txt')\n",
    "\n",
    "domains = set(record['Domain'] for record in all_data)\n",
    "ood_domains = select_ood_domains(domains, 1)  # Using 1 as hardcoded number of OOD domains\n",
    "\n",
    "# Separate OOD data based on domain\n",
    "in_domain_data = [record for record in all_data if record['Domain'] not in ood_domains]\n",
    "ood_data = [record for record in all_data if record['Domain'] in ood_domains]\n",
    "\n",
    "intent_counts = Counter(record['Intent'] for record in in_domain_data)\n",
    "\n",
    "# Filter intents with at least 10 instances\n",
    "sufficient_data = [record for record in in_domain_data if intent_counts[record['Intent']] > 10]\n",
    "\n",
    "train_val_data, test_data = train_test_split(\n",
    "    sufficient_data, test_size=0.2, random_state=seed_value, stratify=[record['Intent'] for record in sufficient_data]\n",
    ")\n",
    "train_data, val_data = train_test_split(\n",
    "    train_val_data, test_size=0.125, random_state=seed_value, stratify=[record['Intent'] for record in train_val_data]\n",
    ")\n",
    "\n",
    "# Split OOD data between validation and test\n",
    "oos_val_data, oos_test_data = train_test_split(ood_data, test_size=0.67, random_state=seed_value)  # No stratification here since it's all OOD\n",
    "\n",
    "# Extract sentences and labels\n",
    "train_sentences = [record['Utterance'] for record in train_data]\n",
    "train_labels = [record['Intent'] for record in train_data]\n",
    "\n",
    "val_sentences = [record['Utterance'] for record in val_data]\n",
    "val_labels = [record['Intent'] for record in val_data]\n",
    "\n",
    "test_sentences = [record['Utterance'] for record in test_data]\n",
    "test_labels = [record['Intent'] for record in test_data]\n",
    "\n",
    "oos_val_sentences = [record['Utterance'] for record in oos_val_data]\n",
    "oos_test_sentences = [record['Utterance'] for record in oos_test_data]\n",
    "\n",
    "# Summary of splits\n",
    "summary = {\n",
    "    \"OOD Domains\": ood_domains,\n",
    "    \"Train Set Size\": len(train_sentences),\n",
    "    \"Validation Set Size\": len(val_sentences),\n",
    "    \"Test Set Size\": len(test_sentences),\n",
    "    \"OOS Validation Set Size\": len(oos_val_sentences),\n",
    "    \"OOS Test Set Size\": len(oos_test_sentences)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"ce_model_bert_mtop.pth\"\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc5e75-4a0a-47f7-9706-d0a6acd42728",
   "metadata": {},
   "source": [
    "# 20NewsGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc91fd07-31bd-4c90-99f3-f9ea1f9d188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# # Load the dataset\n",
    "# newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=seed_value)\n",
    "# def extract_and_clean_subject(text):\n",
    "#     # Search for the subject line\n",
    "#     match = re.search(r'Subject: (.+)', text)\n",
    "#     if not match:\n",
    "#         return \"no subject found\"\n",
    "    \n",
    "#     # Extract the subject and remove leading \"Re: \" if present\n",
    "#     subject = match.group(1)\n",
    "#     cleaned_subject = re.sub(r'^\\s*Re:\\s*', '', subject, flags=re.IGNORECASE)\n",
    "    \n",
    "#     # Return the cleaned subject in lowercase\n",
    "#     return cleaned_subject.lower()\n",
    "# # Data and labels\n",
    "# data = newsgroups.data\n",
    "# data = [extract_and_clean_subject(doc) for doc in data]\n",
    "# labels = newsgroups.target\n",
    "# target_names = newsgroups.target_names\n",
    "\n",
    "# # Define OOD categories (any category that includes 'misc')\n",
    "# ood_categories = [name for name in target_names if 'misc.forsale' in name]\n",
    "# ood_indices = [target_names.index(cat) for cat in ood_categories]\n",
    "# id_indices = [i for i in range(len(target_names)) if i not in ood_indices]\n",
    "\n",
    "# # Filter ID and OOD data\n",
    "# id_data = [data[i] for i in range(len(data)) if labels[i] in id_indices]\n",
    "# id_labels = [labels[i] for i in range(len(data)) if labels[i] in id_indices]\n",
    "# ood_data = [data[i] for i in range(len(data)) if labels[i] in ood_indices]\n",
    "\n",
    "# # Split ID data into train, validation, and test\n",
    "# train_sentences, temp_data, train_labels, temp_labels = train_test_split(\n",
    "#     id_data, id_labels, test_size=0.3, random_state=seed_value)\n",
    "# val_sentences, test_sentences, val_labels, test_labels = train_test_split(\n",
    "#     temp_data, temp_labels, test_size=0.5, random_state=seed_value)\n",
    "\n",
    "# # Assume we use the OOD data only for validation and testing\n",
    "# oos_val_sentences, oos_test_sentences = train_test_split(ood_data, test_size=0.5, random_state=42)\n",
    "# model_name = \"ce_model_bert_20newsgroup.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddacf8-4989-4240-b5d8-665fffa41bdd",
   "metadata": {},
   "source": [
    "# SearchSnippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fd4d3a-87fd-44a3-8974-11f01800008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the files\n",
    "# text_path = 'search_snippets/SearchSnippets.txt'\n",
    "# label_path = 'search_snippets/SearchSnippets_label.txt'\n",
    "\n",
    "# # Load sentences and labels\n",
    "# with open(text_path, 'r', encoding='utf-8') as file:\n",
    "#     sentences = file.readlines()\n",
    "# with open(label_path, 'r', encoding='utf-8') as file:\n",
    "#     labels = [int(line.strip()) for line in file.readlines()]\n",
    "\n",
    "# # Split data into training and temporary (val + test)\n",
    "# train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n",
    "#     sentences, labels, test_size=0.35, random_state=seed_value)\n",
    "\n",
    "# # Split temporary data into validation and test\n",
    "# val_sentences, test_sentences, val_labels, test_labels = train_test_split(\n",
    "#     temp_sentences, temp_labels, test_size=0.5, random_state=seed_value)\n",
    "\n",
    "# # Randomly select 2 classes as OOD classes, rest as ID classes\n",
    "# all_classes = list(range(1, 9))  # Assuming classes are labeled from 1 to 8\n",
    "# ood_classes = set(random.sample(all_classes, 2))\n",
    "# id_classes = set(all_classes) - ood_classes\n",
    "\n",
    "# # Function to filter sentences by class\n",
    "# def filter_by_class(sentences, labels, classes):\n",
    "#     return [s for s, l in zip(sentences, labels) if l in classes]\n",
    "\n",
    "# # Populate ID and OOD datasets\n",
    "# id_train_sentences = filter_by_class(train_sentences, train_labels, id_classes)\n",
    "# id_train_labels = [l for l in train_labels if l in id_classes]\n",
    "# id_val_sentences = filter_by_class(val_sentences, val_labels, id_classes)\n",
    "# id_val_labels = [l for l in val_labels if l in id_classes]\n",
    "# id_test_sentences = filter_by_class(test_sentences, test_labels, id_classes)\n",
    "# id_test_labels = [l for l in test_labels if l in id_classes]\n",
    "\n",
    "# oos_val_sentences = filter_by_class(val_sentences, val_labels, ood_classes)\n",
    "# oos_test_sentences = filter_by_class(test_sentences, test_labels, ood_classes)\n",
    "\n",
    "# # Assign the filtered lists back to the main variables\n",
    "# train_sentences, train_labels = id_train_sentences, id_train_labels\n",
    "# val_sentences, val_labels = id_val_sentences, id_val_labels\n",
    "# test_sentences, test_labels = id_test_sentences, id_test_labels\n",
    "# model_name = \"ce_model_bert_search_snippets.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b949826-2039-4f7c-bd8e-93c4bab84bba",
   "metadata": {},
   "source": [
    "# Cerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c98ffe6-73df-48fb-a584-7ad414baab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ood_suffix = \"just_talk_embedded_reject_twitter_blind-testonly.en-US.NCS5.1.final_1.input\"\n",
    "# def load_sentences_and_labels(data_dir, subset, in_system=True, suffix=\".input\"):\n",
    "#     \"\"\"\n",
    "#     Load sentences and their labels from a given directory.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - data_dir: Base directory containing the data subsets ('dev', 'test', 'train').\n",
    "#     - subset: Subset of data to load ('dev', 'train', 'test').\n",
    "#     - in_system: Boolean indicating whether to load InSys (True) or OOS (False) data.\n",
    "#     - suffix: File suffix to look for.\n",
    "    \n",
    "#     Returns:\n",
    "#     - sentences: List of sentences loaded from the files.\n",
    "#     - labels: List of corresponding labels derived from file names.\n",
    "#     \"\"\"\n",
    "#     sentences = []\n",
    "#     labels = []\n",
    "#     subset_dir = \"InSys\" if in_system else \"OOS\"\n",
    "#     full_path = os.path.join(data_dir, subset, subset_dir)\n",
    "    \n",
    "#     for filename in os.listdir(full_path):\n",
    "#         if filename.endswith(suffix):\n",
    "#             label = filename.replace(suffix, '')\n",
    "#             with open(os.path.join(full_path, filename), 'r', encoding='utf-8') as file:\n",
    "#                 file_sentences = file.readlines()\n",
    "#                 sentences.extend([sentence.strip() for sentence in file_sentences])\n",
    "#                 labels.extend([label] * len(file_sentences))\n",
    "    \n",
    "#     return sentences, labels\n",
    "\n",
    "# # Specify the base directory where your 'dev', 'test', 'train' directories are located\n",
    "# base_dir = 'cerence_data'\n",
    "\n",
    "# # Load the datasets\n",
    "# train_sentences, train_labels = load_sentences_and_labels(base_dir, 'train', suffix=\".train.input\")\n",
    "# val_sentences, val_labels = load_sentences_and_labels(base_dir, 'dev', suffix=\".dev.input\")\n",
    "# test_sentences, test_labels = load_sentences_and_labels(base_dir, 'test', in_system=True, suffix=\".test.input\")\n",
    "# oos_test_sentences, _ = load_sentences_and_labels(base_dir, 'test', in_system=False, suffix=ood_suffix)\n",
    "\n",
    "# # Split half as val\n",
    "# random.shuffle(oos_test_sentences)\n",
    "# split_index = len(oos_test_sentences) // 2\n",
    "\n",
    "# # Split the sentences into two halves\n",
    "# oos_val_sentences = oos_test_sentences[:split_index]\n",
    "# oos_test_sentences = oos_test_sentences[split_index:]\n",
    "\n",
    "# train_sentences, _, train_labels, _ = train_test_split(\n",
    "#     train_sentences, train_labels, train_size=1/25, stratify=train_labels, random_state=seed_value)\n",
    "# test_sentences, _, test_labels, _ = train_test_split(\n",
    "#     test_sentences, test_labels, train_size=1/20, stratify=test_labels, random_state=seed_value)\n",
    "\n",
    "# model_name = \"ce_model_bert_cerence.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c172e5b-fc0e-45cb-9821-35a0499aa792",
   "metadata": {},
   "source": [
    "# Load CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:06.596136455Z",
     "start_time": "2023-12-01T05:47:06.586541155Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load the dataset\n",
    "# with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "#     data = json.load(file)\n",
    "# # Extracting data\n",
    "# train_data = data['train']\n",
    "# val_data = data['val']\n",
    "# test_data = data['test']\n",
    "\n",
    "# oos_train_data = data['oos_train']\n",
    "# oos_val_data = data['oos_val']\n",
    "# oos_test_data = data['oos_test']\n",
    "\n",
    "# # Get sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# model_name = \"ce_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a8d54-b729-462f-a3d3-4ed5792453e3",
   "metadata": {},
   "source": [
    "# BANKING77-OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2921501a-20fa-4392-8e60-91c0400e7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test',\n",
    "#     'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "#     'oos_test': f'{base_dir}/ood-oos/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "# oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "# oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "# model_name = \"ce_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cae5e-afa7-45ec-9dfd-b4201c9c4b66",
   "metadata": {},
   "source": [
    "# SNIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2283c7-a17b-44ac-8930-c073ceb462b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/SNIPS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test'\n",
    "# }\n",
    "# # Define paths to the dataset directories\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# def extract_sentences_labels(dataset):\n",
    "#     sentences = [e.text for e in dataset]\n",
    "#     labels = [e.label for e in dataset]\n",
    "#     return sentences, labels\n",
    "\n",
    "# all_train_sentences, all_train_labels = extract_sentences_labels(datasets['train'])\n",
    "# all_val_sentences, all_val_labels = extract_sentences_labels(datasets['valid'])\n",
    "# all_test_sentences, all_test_labels = extract_sentences_labels(datasets['test'])\n",
    "\n",
    "# # Count the frequency of each label in the training data\n",
    "# label_freq = collections.Counter(all_train_labels)\n",
    "\n",
    "# # Calculate the threshold for selecting 75% of the data as in-domain\n",
    "# threshold = len(all_train_labels) * 0.75\n",
    "# cumulative_count = 0\n",
    "# selected_labels = []\n",
    "\n",
    "# # Randomize the label order for fairness\n",
    "# all_labels = list(label_freq.keys())\n",
    "# random.shuffle(all_labels)\n",
    "\n",
    "# for label in all_labels:\n",
    "#     count = label_freq[label]\n",
    "#     cumulative_count += count\n",
    "#     selected_labels.append(label)\n",
    "#     if cumulative_count >= threshold:\n",
    "#         break\n",
    "\n",
    "# # Function to filter data by selected labels\n",
    "# def filter_data(sentences, labels, selected_labels):\n",
    "#     filtered_data = [(s, l) for s, l in zip(sentences, labels) if l in selected_labels]\n",
    "#     return [item[0] for item in filtered_data], [item[1] for item in filtered_data]\n",
    "\n",
    "# # Filter the datasets\n",
    "# train_sentences, train_labels = filter_data(all_train_sentences, all_train_labels, selected_labels)\n",
    "# val_sentences, val_labels = filter_data(all_val_sentences, all_val_labels, selected_labels)\n",
    "# test_sentences, test_labels = filter_data(all_test_sentences, all_test_labels, selected_labels)\n",
    "\n",
    "# # Identify and separate the out-of-domain data\n",
    "# oos_labels = set(all_labels) - set(selected_labels)\n",
    "# oos_train_sentences, oos_train_labels = filter_data(all_train_sentences, all_train_labels, oos_labels)\n",
    "# oos_val_sentences, oos_val_labels = filter_data(all_val_sentences, all_val_labels, oos_labels)\n",
    "# oos_test_sentences, oos_test_labels = filter_data(all_test_sentences, all_test_labels, oos_labels)\n",
    "\n",
    "# # Evenly split oos_train data between validation and test\n",
    "# half_index = len(oos_train_sentences) // 2\n",
    "# oos_val_sentences.extend(oos_train_sentences[:half_index])\n",
    "# oos_val_labels.extend(oos_train_labels[:half_index])\n",
    "# oos_test_sentences.extend(oos_train_sentences[half_index:])\n",
    "# oos_test_labels.extend(oos_train_labels[half_index:])\n",
    "# model_name = \"ce_model_bert_SNIP.pth\"\n",
    "# # oos_scenario is no longer a single scenario but a list of OOD labels\n",
    "# oos_scenarios = list(oos_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7cef97-b078-44d3-82be-022e2175e083",
   "metadata": {},
   "source": [
    "# ROSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bfb428f-d733-4d62-8c35-d2d4234342cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"cmaldona/Generalization-MultiClass-CLINC150-ROSTD\", \"rostd+\")\n",
    "\n",
    "# train_sentences = []\n",
    "# train_labels = []\n",
    "# val_sentences = []\n",
    "# val_labels = []\n",
    "# test_sentences = []\n",
    "# test_labels = []\n",
    "# oos_test_sentences = []\n",
    "\n",
    "# # Extract training data\n",
    "# for example in dataset['train']:\n",
    "#     train_sentences.append(example['data'].lower())\n",
    "#     train_labels.append(example['labels'])\n",
    "\n",
    "# # Extract validation data\n",
    "# for example in dataset['validation']:\n",
    "#     val_sentences.append(example['data'].lower())\n",
    "#     val_labels.append(example['labels'])\n",
    "\n",
    "# # Extract test data and separate ID from OOS\n",
    "# for example in dataset['test']:\n",
    "#     if example['generalisation'] == 'ID':\n",
    "#         test_sentences.append(example['data'].lower())\n",
    "#         test_labels.append(example['labels'])\n",
    "#     elif example['generalisation'] == 'near-OOD' or example['generalisation'] == 'far-OOD':# OOS\n",
    "#         try:\n",
    "#             oos_test_sentences.append(example['data'].lower())\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "\n",
    "# model_name = \"ce_model_bert_ROSTD.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a254b19-b90b-41ea-bc3f-849181fd4884",
   "metadata": {},
   "source": [
    "# StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8fca9a-a366-4854-ba19-cda92357c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_number = 1\n",
    "\n",
    "# base_dir = 'stackoverflow_data'  \n",
    "\n",
    "# # Construct the path to the specific split\n",
    "# split_dir = os.path.join(base_dir, f'split{split_number}')\n",
    "\n",
    "# # Function to load data from a .pkl file\n",
    "# def load_data_from_split(split_dir, file_name):\n",
    "#     with open(os.path.join(split_dir, file_name), 'rb') as file:\n",
    "#         return pickle.load(file)\n",
    "\n",
    "# # Load the datasets\n",
    "# train_sentences = load_data_from_split(split_dir, 'train_sentences.pkl')\n",
    "# train_labels = load_data_from_split(split_dir, 'train_labels.pkl')\n",
    "# val_sentences = load_data_from_split(split_dir, 'val_sentences.pkl')\n",
    "# val_labels = load_data_from_split(split_dir, 'val_labels.pkl')\n",
    "# test_sentences = load_data_from_split(split_dir, 'test_sentences.pkl')\n",
    "# test_labels = load_data_from_split(split_dir, 'test_labels.pkl')\n",
    "# oos_val_sentences = load_data_from_split(split_dir, 'oos_val_sentences.pkl')\n",
    "# oos_test_sentences = load_data_from_split(split_dir, 'oos_test_sentences.pkl')\n",
    "# model_name = \"ce_model_bert_Stack.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355048d2-2392-4029-a853-23dba8eb9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{seed_value}_{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)\n",
    "encoded_test_labels = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 41\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)\n",
    "test_dataset = TextDataset(test_sentences, encoded_test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the output from the transformer model\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Pool the outputs into a single sentence vector\n",
    "        # You can use `transformer_output.last_hidden_state.mean(dim=1)` for mean pooling\n",
    "        # or `transformer_output.last_hidden_state.max(dim=1).values` for max pooling\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "        # Forward pass through the classifier layer\n",
    "        return self.classifier(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=2.25E-05)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:02.079280530Z",
     "start_time": "2023-12-01T05:54:02.071418245Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:20.768410345Z",
     "start_time": "2023-12-01T05:54:06.212110157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Lower validation loss found. Model saved.\n",
      "Epoch 1/10, Training Loss: 1.86166e+00, Validation Loss: 7.22584e-01\n",
      "Epoch 2/10: Lower validation loss found. Model saved.\n",
      "Epoch 2/10, Training Loss: 5.47760e-01, Validation Loss: 3.59231e-01\n",
      "Epoch 3/10: Lower validation loss found. Model saved.\n",
      "Epoch 3/10, Training Loss: 2.94018e-01, Validation Loss: 2.28093e-01\n",
      "Epoch 4/10: Lower validation loss found. Model saved.\n",
      "Epoch 4/10, Training Loss: 1.82920e-01, Validation Loss: 1.74819e-01\n",
      "Epoch 5/10: Lower validation loss found. Model saved.\n",
      "Epoch 5/10, Training Loss: 1.15361e-01, Validation Loss: 1.54268e-01\n",
      "Epoch 6/10: Lower validation loss found. Model saved.\n",
      "Epoch 6/10, Training Loss: 8.03761e-02, Validation Loss: 1.34471e-01\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_name):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            predictions = model(input_ids, attention_mask)  # Forward pass\n",
    "            loss = loss_function(predictions, labels) \n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss) \n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                predictions = model(input_ids, attention_mask) \n",
    "                loss = loss_function(predictions, labels)  # Compute loss\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")\n",
    "else:\n",
    "        print(\"training skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:29.223470776Z",
     "start_time": "2023-12-01T05:55:29.126474189Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf64b38465b810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:57:29.484254207Z",
     "start_time": "2023-12-01T05:57:29.442532320Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get trasformer part of the model\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:00.612845683Z",
     "start_time": "2023-12-01T05:57:30.186777749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "val_embeddings = encode_sentences(fine_model, val_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_val_embeddings = encode_sentences(fine_model, oos_val_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c0396-bdbf-47e5-8071-63001dea2d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:03.735123310Z",
     "start_time": "2023-12-01T05:58:03.728059325Z"
    }
   },
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f9fe7-4b7f-41d7-a81d-f1cf570254eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:10.268016161Z",
     "start_time": "2023-12-01T05:58:09.663593617Z"
    }
   },
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f854ee-b40a-452a-82b6-687e6629a9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:11.061843439Z",
     "start_time": "2023-12-01T05:58:11.036326005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba7dac-3784-4b15-b1d0-b171099d3b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:13.108176906Z",
     "start_time": "2023-12-01T05:58:13.089972609Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3237a-cad6-417c-b216-e2735015e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = val_scores + oos_val_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b8398-4a6f-44bd-99a3-1d929584d5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:23.790010744Z",
     "start_time": "2023-12-01T05:58:14.102565724Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe762a-5fdf-43a8-bd6d-c9e7042e5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2ec4f-a67f-4497-8864-b99cb798876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask) \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())  # Move preds to CPU and convert to numpy\n",
    "            real_labels.extend(labels.detach().cpu().numpy())  # Move labels to CPU and convert to numpy\n",
    "    return predictions, real_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb390c89-82dd-4085-9e58-0d69c8370cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model = fine_model.to(device)\n",
    "preds, true_labels = get_predictions(fine_model, test_dataloader)\n",
    "\n",
    "# Now you can calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "print(f\"Classification Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bf5ab-e718-4a7a-a710-5512372b069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:30.123817339Z",
     "start_time": "2023-12-01T05:58:29.877381627Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(val_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_val_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels \n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dba537-bb6b-491b-8020-0343301d81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores,bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1037062-1be6-4d85-9b69-98248ef45ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_labels = np.unique(test_labels)\n",
    "# num_unique_labels = len(unique_labels)\n",
    " \n",
    "# ood_label = \"ood\"\n",
    " \n",
    "# # Combine embeddings\n",
    "# combined_embeddings = np.vstack((test_embeddings, oos_test_embeddings))\n",
    " \n",
    "# # Create labels for combined data\n",
    "# combined_labels = np.concatenate((test_labels, np.array([ood_label] * len(oos_test_embeddings))))\n",
    " \n",
    "# # TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=25)\n",
    "# embeddings_2d = tsne.fit_transform(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b25c32-c027-40b1-93cf-d3ee8af00d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# unique_labels = np.unique(combined_labels)\n",
    "# for label in unique_labels:\n",
    "#     indices = np.where(combined_labels == label)[0]\n",
    "#     if label == ood_label:\n",
    "#         # Specific color for OOD\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='black', s=0.5, alpha=0.7)\n",
    "#     else:\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='red', s=0.5, alpha=0.7)\n",
    "# plt.title('2D t-SNE Plot of Embeddings (Baseline)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Assuming other necessary imports are already there\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Optuna suggests hyperparameters\n",
    "#     seed_value=42\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "#     torch.cuda.manual_seed_all(seed_value)\n",
    "#     lr = trial.suggest_categorical('lr', [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2])\n",
    "#     num_epochs = 25\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "#     training_losses = []\n",
    "#     validation_losses = []\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     # Model setup\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "#     transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "#     model = TextClassifier(transformer_model, len(unique_intents))\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "    \n",
    "\n",
    "#     # Loss function and optimizer\n",
    "#     loss_function = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training Phase\n",
    "#         model.train()  # Set the model to training mode\n",
    "#         total_train_loss = 0\n",
    "#         for batch in train_dataloader:\n",
    "#             input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "#             optimizer.zero_grad()  # Zero the gradients\n",
    "#             predictions = model(input_ids, attention_mask)  # Forward pass\n",
    "#             loss = loss_function(predictions, labels) \n",
    "#             loss.backward()  # Backward pass\n",
    "#             optimizer.step()  # Update weights\n",
    "    \n",
    "#             total_train_loss += loss.item()\n",
    "        \n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         training_losses.append(avg_train_loss) \n",
    "    \n",
    "#         # Validation Phase\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         total_val_loss = 0\n",
    "#         with torch.no_grad():  # Disable gradient calculations\n",
    "#             for batch in val_dataloader:\n",
    "#                 input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "#                 predictions = model(input_ids, attention_mask) \n",
    "#                 loss = loss_function(predictions, labels)  # Compute loss\n",
    "#                 total_val_loss += loss.item()\n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "#         validation_losses.append(avg_val_loss)\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             # Save the model\n",
    "#             torch.save(model, model_name)\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "#     return best_val_loss\n",
    "    \n",
    "#     # trial.set_user_attr(\"training_losses\", training_losses)\n",
    "#     # trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "#     # model = torch.load(model_name)\n",
    "#     # model.eval()  # Put the model in evaluation mode\n",
    "#     # model = fine_model.to(device)\n",
    "#     # fine_transformer = model.transformer\n",
    "#     # train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "#     # val_embeddings = encode_sentences(fine_transformer, val_sentences)\n",
    "#     # oos_val_embeddings = encode_sentences(fine_transformer, oos_val_sentences)\n",
    "\n",
    "#     # intent_means = {}\n",
    "#     # for encoded_label in np.unique(encoded_train_labels):\n",
    "#     #     # Find indices where the encoded label matches\n",
    "#     #     indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "#     #     # Calculate the mean embedding for the current intent\n",
    "#     #     intent_embeddings = train_embeddings[indices]\n",
    "#     #     intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "#     #     # Use the encoded label as the dictionary key\n",
    "#     #     intent_means[encoded_label] = intent_mean\n",
    "#     # covariance = np.cov(train_embeddings, rowvar=False)\n",
    "#     # cov_inverse = inv(covariance)\n",
    "#     # val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "#     # oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "#     # # True binary labels: 0 for in-domain and 1 for OOD\n",
    "#     # y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "#     # # Combine the scores\n",
    "#     # y_scores = val_scores + oos_val_scores\n",
    "\n",
    "#     # # Compute AUPR\n",
    "#     # aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "#     # return aupr\n",
    "\n",
    "# # Create a study object and optimize the objective function\n",
    "# study = optuna.create_study(direction='minimize',  study_name='ce_loss_CLINC150_min_val', storage='sqlite:///desperate.db', load_if_exists= True)\n",
    "# study.optimize(objective, n_trials=35)  # n_trials is the number of iterations\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = study.best_params\n",
    "# print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4255a-be76-4f70-971c-5f4d91d5262a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
