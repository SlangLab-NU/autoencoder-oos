{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "40499132-518f-4f48-a389-f5a281434e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=10912\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693267d-9a81-4282-b9bb-d853102347c5",
   "metadata": {},
   "source": [
    "# Load CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:06.596136455Z",
     "start_time": "2023-12-01T05:47:06.586541155Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load the dataset\n",
    "# with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "#     data = json.load(file)\n",
    "# # Extracting data\n",
    "# train_data = data['train']\n",
    "# val_data = data['val']\n",
    "# test_data = data['test']\n",
    "\n",
    "# oos_train_data = data['oos_train']\n",
    "# oos_val_data = data['oos_val']\n",
    "# oos_test_data = data['oos_test']\n",
    "\n",
    "# # Get sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# model_name = \"improved_ce_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54766b2c-cd88-4636-bff0-ba15c4272f11",
   "metadata": {},
   "source": [
    "# Load SLURP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "88a73b50-057e-4813-8606-bed9789c138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_path):\n",
    "#     sentences = []\n",
    "#     scenarios = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             sentence = data.get('sentence', None)\n",
    "#             scenario = data.get('scenario', None)\n",
    "#             if sentence is not None and scenario is not None:\n",
    "#                 sentences.append(sentence)\n",
    "#                 scenarios.append(scenario)\n",
    "#     return sentences, scenarios\n",
    "\n",
    "# # Randomly select one domain to be out of scope\n",
    "# unique_scenarios = {'alarm', 'audio', 'calendar', 'cooking', 'datetime', 'email', 'general', 'iot', 'lists', 'music', 'news', 'play', 'qa', 'recommendation', 'social', 'takeaway', 'transport', 'weather'}\n",
    "# oos_scenario = random.choice(list(unique_scenarios))\n",
    "\n",
    "# # Load data from files\n",
    "# train_sentences, train_labels = load_data('slurp/dataset/slurp/train.jsonl')\n",
    "# val_sentences, val_labels = load_data('slurp/dataset/slurp/devel.jsonl')\n",
    "# test_sentences, test_labels = load_data('slurp/dataset/slurp/test.jsonl')\n",
    "\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "# model_name = \"improved_ce_model_bert_SLURP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c9cf7-222d-4195-ae55-1fee13ab45b5",
   "metadata": {},
   "source": [
    "# Load Banking77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dd6c0847-fc1c-4964-96a7-7ffea08c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test',\n",
    "#     'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "#     'oos_test': f'{base_dir}/ood-oos/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "# oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "# oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "# model_name = \"improved_ce_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885f3de-13b4-4e6f-900f-5ed5947ea7da",
   "metadata": {},
   "source": [
    "# SNIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bbc0dbd1-2cfe-45a0-9912-6c61d111b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/SNIPS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "# unique_scenarios = set(train_labels)\n",
    "# # oos_scenario = random.choice(list(unique_scenarios))\n",
    "# oos_scenario = 'AddToPlaylist'\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "\n",
    "# model_name = \"improved_ce_model_bert_SNIP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fd985-bb3f-4935-bc1a-6dbf39f3b175",
   "metadata": {},
   "source": [
    "# ROSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7e50a8ee-c7d3-40f0-b7e9-0de11a40435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cmaldona/Generalization-MultiClass-CLINC150-ROSTD\", \"rostd+\")\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "oos_test_sentences = []\n",
    "\n",
    "# Extract training data\n",
    "for example in dataset['train']:\n",
    "    train_sentences.append(example['data'].lower())\n",
    "    train_labels.append(example['labels'])\n",
    "\n",
    "# Extract validation data\n",
    "for example in dataset['validation']:\n",
    "    val_sentences.append(example['data'].lower())\n",
    "    val_labels.append(example['labels'])\n",
    "\n",
    "# Extract test data and separate ID from OOS\n",
    "for example in dataset['test']:\n",
    "    if example['generalisation'] == 'ID':\n",
    "        test_sentences.append(example['data'].lower())\n",
    "        test_labels.append(example['labels'])\n",
    "    elif example['generalisation'] == 'near-OOD' or example['generalisation'] == 'far-OOD':# OOS\n",
    "        try:\n",
    "            oos_test_sentences.append(example['data'].lower())\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "\n",
    "model_name = \"improved_ce_model_bert_ROSTD.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "af8c8984-2fc0-4dd1-bbaa-4d1b459090aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 27\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences):\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_input = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}  # Move input to GPU if available\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output.cpu().numpy())\n",
    "\n",
    "    sentence_embeddings_np = np.concatenate(sentence_embeddings, axis=0)\n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "\n",
    "        # Forward pass through the classifier layer\n",
    "        logits = self.classifier(sentence_embedding)\n",
    "        \n",
    "        return logits, sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5e-06)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a955694f-88a8-457e-b14a-53a9c91d4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def euclidean_distance_loss(embeddings):\n",
    "#     n, k = embeddings.size()\n",
    "#     mean_embeddings = embeddings.mean(dim=0)\n",
    "#     distances = embeddings - mean_embeddings\n",
    "#     loss = (distances ** 2).sum(dim=1).mean() / k\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "54d902b4-6d3c-4963-bfa5-9377cdda0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(embeddings):\n",
    "    n, k = embeddings.size()  # n is the batch size, k is the embedding dimension\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Calculate the mean embedding for each sample, excluding the sample itself\n",
    "    for i in range(n):\n",
    "        # Use indexing to exclude the current sample, then calculate the mean of the remaining samples\n",
    "        indices = [j for j in range(n) if j != i]\n",
    "        mean_embedding = embeddings[indices].mean(dim=0)\n",
    "        \n",
    "        # Calculate the squared Euclidean distance for the current sample\n",
    "        distance = (embeddings[i] - mean_embedding).pow(2).sum()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        loss += distance\n",
    "    \n",
    "    # Average the loss over all samples and divide by the dimension k\n",
    "    loss = loss / (n * k)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:02.079280530Z",
     "start_time": "2023-12-01T05:54:02.071418245Z"
    }
   },
   "outputs": [],
   "source": [
    "ed_loss_importance = 0.12\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:20.768410345Z",
     "start_time": "2023-12-01T05:54:06.212110157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Lower validation loss found. Model saved.\n",
      "Epoch 1/20, Training Loss: 1.20224e+00, Validation Loss: 3.65858e-01\n",
      "Epoch 2/20: Lower validation loss found. Model saved.\n",
      "Epoch 2/20, Training Loss: 2.43174e-01, Validation Loss: 1.26266e-01\n",
      "Epoch 3/20: Lower validation loss found. Model saved.\n",
      "Epoch 3/20, Training Loss: 1.16621e-01, Validation Loss: 9.74854e-02\n",
      "Epoch 4/20: Lower validation loss found. Model saved.\n",
      "Epoch 4/20, Training Loss: 8.86680e-02, Validation Loss: 8.61721e-02\n",
      "Epoch 5/20: Lower validation loss found. Model saved.\n",
      "Epoch 5/20, Training Loss: 7.55896e-02, Validation Loss: 7.92952e-02\n",
      "Epoch 6/20: Lower validation loss found. Model saved.\n",
      "Epoch 6/20, Training Loss: 6.68557e-02, Validation Loss: 7.61717e-02\n",
      "Epoch 7/20: Lower validation loss found. Model saved.\n",
      "Epoch 7/20, Training Loss: 6.02704e-02, Validation Loss: 7.18658e-02\n",
      "Epoch 8/20: Lower validation loss found. Model saved.\n",
      "Epoch 8/20, Training Loss: 5.59513e-02, Validation Loss: 6.96093e-02\n",
      "Epoch 9/20: Lower validation loss found. Model saved.\n",
      "Epoch 9/20, Training Loss: 5.22643e-02, Validation Loss: 6.77342e-02\n",
      "Epoch 10/20, Training Loss: 4.90520e-02, Validation Loss: 6.82539e-02\n",
      "Epoch 11/20: Lower validation loss found. Model saved.\n",
      "Epoch 11/20, Training Loss: 4.67675e-02, Validation Loss: 6.65711e-02\n",
      "Epoch 12/20: Lower validation loss found. Model saved.\n",
      "Epoch 12/20, Training Loss: 4.43467e-02, Validation Loss: 6.56064e-02\n",
      "Epoch 13/20, Training Loss: 4.35193e-02, Validation Loss: 6.66138e-02\n",
      "Epoch 14/20: Lower validation loss found. Model saved.\n",
      "Epoch 14/20, Training Loss: 4.12255e-02, Validation Loss: 6.25988e-02\n",
      "Epoch 15/20, Training Loss: 4.01571e-02, Validation Loss: 6.92346e-02\n",
      "Epoch 16/20, Training Loss: 3.86046e-02, Validation Loss: 6.31975e-02\n",
      "Epoch 17/20, Training Loss: 3.77048e-02, Validation Loss: 6.67920e-02\n",
      "Epoch 18/20, Training Loss: 3.67418e-02, Validation Loss: 6.34174e-02\n",
      "Epoch 19/20, Training Loss: 3.53006e-02, Validation Loss: 6.39631e-02\n",
      "Epoch 20/20, Training Loss: 3.47429e-02, Validation Loss: 6.61900e-02\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('Inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "        ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "        ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "        total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "        \n",
    "        total_loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss) \n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            total_val_loss += total_loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model\n",
    "        torch.save(model, model_name)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:29.223470776Z",
     "start_time": "2023-12-01T05:55:29.126474189Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "daf64b38465b810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:57:29.484254207Z",
     "start_time": "2023-12-01T05:57:29.442532320Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get trasformer part of the model\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:00.612845683Z",
     "start_time": "2023-12-01T05:57:30.186777749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ec7c0396-bdbf-47e5-8071-63001dea2d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:03.735123310Z",
     "start_time": "2023-12-01T05:58:03.728059325Z"
    }
   },
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cf9f9fe7-4b7f-41d7-a81d-f1cf570254eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:10.268016161Z",
     "start_time": "2023-12-01T05:58:09.663593617Z"
    }
   },
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f3f854ee-b40a-452a-82b6-687e6629a9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:11.061843439Z",
     "start_time": "2023-12-01T05:58:11.036326005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "27ba7dac-3784-4b15-b1d0-b171099d3b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:13.108176906Z",
     "start_time": "2023-12-01T05:58:13.089972609Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "425b8398-4a6f-44bd-99a3-1d929584d5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:23.790010744Z",
     "start_time": "2023-12-01T05:58:14.102565724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875794937353319"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8cfe1c36-a25f-4b33-a787-36ec4fe62f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992555164589185"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0b5bf5ab-e718-4a7a-a710-5512372b069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:30.123817339Z",
     "start_time": "2023-12-01T05:58:29.877381627Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Assuming other necessary imports are already there\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Optuna suggests hyperparameters\n",
    "#     seed_value=42\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "#     torch.cuda.manual_seed_all(seed_value)\n",
    "#     lr = trial.suggest_float('lr', 5e-6,1e-3, log=True)\n",
    "#     num_epochs = trial.suggest_int('num_epochs', 5, 20)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "#     ed_loss_importance = trial.suggest_float('ed_loss_importance', 0, 1)\n",
    "#     training_losses = []\n",
    "#     validation_losses = []\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     # Model setup\n",
    "#     num_labels = 150\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "#     transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "#     model = TextClassifier(transformer_model, num_labels)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "    \n",
    "\n",
    "#     # Loss function and optimizer\n",
    "#     loss_function = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training Phase\n",
    "#         model.train()  # Set the model to training mode\n",
    "#         total_train_loss = 0\n",
    "#         for batch in train_dataloader:\n",
    "#             input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "#             optimizer.zero_grad()  # Zero the gradients\n",
    "#             predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "#             ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "#             ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "#             total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            \n",
    "#             total_loss.backward()  # Backward pass\n",
    "#             optimizer.step()  # Update weights\n",
    "    \n",
    "#             total_train_loss += total_loss.item()\n",
    "        \n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         training_losses.append(avg_train_loss) \n",
    "    \n",
    "#         # Validation Phase\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         total_val_loss = 0\n",
    "#         with torch.no_grad():  # Disable gradient calculations\n",
    "#             for batch in val_dataloader:\n",
    "#                 input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "#                 predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "#                 ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "#                 ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "#                 total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "#                 total_val_loss += total_loss.item()\n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "#         validation_losses.append(avg_train_loss)\n",
    "#     trial.set_user_attr(\"training_losses\", training_losses)\n",
    "#     trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "#     fine_transformer = model.transformer\n",
    "#     train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "#     test_embeddings = encode_sentences(fine_transformer, test_sentences)\n",
    "#     oos_test_embeddings = encode_sentences(fine_transformer, oos_test_sentences)\n",
    "\n",
    "#     intent_means = {}\n",
    "#     for encoded_label in np.unique(encoded_train_labels):\n",
    "#         # Find indices where the encoded label matches\n",
    "#         indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "#         # Calculate the mean embedding for the current intent\n",
    "#         intent_embeddings = train_embeddings[indices]\n",
    "#         intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "#         # Use the encoded label as the dictionary key\n",
    "#         intent_means[encoded_label] = intent_mean\n",
    "#     covariance = np.cov(train_embeddings, rowvar=False)\n",
    "#     cov_inverse = inv(covariance)\n",
    "#     test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "#     oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "#     # True binary labels: 0 for in-domain and 1 for OOD\n",
    "#     y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "#     # Combine the scores\n",
    "#     y_scores = test_scores + oos_test_scores\n",
    "\n",
    "#     # Compute AUPR\n",
    "#     aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "#     return aupr\n",
    "\n",
    "# # Create a study object and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize',  study_name='improved_ce_loss_sniP', storage='sqlite:///desperate2.db')\n",
    "# study.optimize(objective, n_trials=1000)  # n_trials is the number of iterations\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = study.best_params\n",
    "# print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcac44a-52a1-4f17-8544-bee6e9c5d984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
