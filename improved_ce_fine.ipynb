{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40499132-518f-4f48-a389-f5a281434e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value=10912\n",
    "# random.seed(seed_value)\n",
    "# np.random.seed(seed_value)\n",
    "# torch.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693267d-9a81-4282-b9bb-d853102347c5",
   "metadata": {},
   "source": [
    "# Load CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:06.596136455Z",
     "start_time": "2023-12-01T05:47:06.586541155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "# Extracting data\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']\n",
    "\n",
    "oos_train_data = data['oos_train']\n",
    "oos_val_data = data['oos_val']\n",
    "oos_test_data = data['oos_test']\n",
    "\n",
    "# Get sentences and labels\n",
    "train_sentences = [item[0] for item in train_data]\n",
    "train_labels = [item[1] for item in train_data]\n",
    "\n",
    "val_sentences = [item[0] for item in val_data]\n",
    "val_labels = [item[1] for item in val_data]\n",
    "\n",
    "test_sentences = [item[0] for item in test_data]\n",
    "test_labels = [item[1] for item in test_data]\n",
    "\n",
    "oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "model_name = \"improved_ce_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54766b2c-cd88-4636-bff0-ba15c4272f11",
   "metadata": {},
   "source": [
    "# Load SLURP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a73b50-057e-4813-8606-bed9789c138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_path):\n",
    "#     sentences = []\n",
    "#     scenarios = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             sentence = data.get('sentence', None)\n",
    "#             scenario = data.get('scenario', None)\n",
    "#             if sentence is not None and scenario is not None:\n",
    "#                 sentences.append(sentence)\n",
    "#                 scenarios.append(scenario)\n",
    "#     return sentences, scenarios\n",
    "\n",
    "# # Randomly select one domain to be out of scope\n",
    "# unique_scenarios = {'alarm', 'audio', 'calendar', 'cooking', 'datetime', 'email', 'general', 'iot', 'lists', 'music', 'news', 'play', 'qa', 'recommendation', 'social', 'takeaway', 'transport', 'weather'}\n",
    "# oos_scenario = random.choice(list(unique_scenarios))\n",
    "\n",
    "# # Load data from files\n",
    "# train_sentences, train_labels = load_data('slurp/dataset/slurp/train.jsonl')\n",
    "# val_sentences, val_labels = load_data('slurp/dataset/slurp/devel.jsonl')\n",
    "# test_sentences, test_labels = load_data('slurp/dataset/slurp/test.jsonl')\n",
    "\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "# model_name = \"improved_ce_model_bert_SLURP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c9cf7-222d-4195-ae55-1fee13ab45b5",
   "metadata": {},
   "source": [
    "# Load Banking77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6c0847-fc1c-4964-96a7-7ffea08c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test',\n",
    "#     'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "#     'oos_test': f'{base_dir}/ood-oos/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "# oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "# oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "# model_name = \"improved_ce_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885f3de-13b4-4e6f-900f-5ed5947ea7da",
   "metadata": {},
   "source": [
    "# SNIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc0dbd1-2cfe-45a0-9912-6c61d111b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/SNIPS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "# unique_scenarios = set(train_labels)\n",
    "# # oos_scenario = random.choice(list(unique_scenarios))\n",
    "# oos_scenario = 'AddToPlaylist'\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "\n",
    "# model_name = \"improved_ce_model_bert_SNIP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fd985-bb3f-4935-bc1a-6dbf39f3b175",
   "metadata": {},
   "source": [
    "# ROSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e50a8ee-c7d3-40f0-b7e9-0de11a40435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"cmaldona/Generalization-MultiClass-CLINC150-ROSTD\", \"rostd+\")\n",
    "\n",
    "# train_sentences = []\n",
    "# train_labels = []\n",
    "# val_sentences = []\n",
    "# val_labels = []\n",
    "# test_sentences = []\n",
    "# test_labels = []\n",
    "# oos_test_sentences = []\n",
    "\n",
    "# # Extract training data\n",
    "# for example in dataset['train']:\n",
    "#     train_sentences.append(example['data'].lower())\n",
    "#     train_labels.append(example['labels'])\n",
    "\n",
    "# # Extract validation data\n",
    "# for example in dataset['validation']:\n",
    "#     val_sentences.append(example['data'].lower())\n",
    "#     val_labels.append(example['labels'])\n",
    "\n",
    "# # Extract test data and separate ID from OOS\n",
    "# for example in dataset['test']:\n",
    "#     if example['generalisation'] == 'ID':\n",
    "#         test_sentences.append(example['data'].lower())\n",
    "#         test_labels.append(example['labels'])\n",
    "#     elif example['generalisation'] == 'near-OOD' or example['generalisation'] == 'far-OOD':# OOS\n",
    "#         try:\n",
    "#             oos_test_sentences.append(example['data'].lower())\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "\n",
    "# model_name = \"improved_ce_model_bert_ROSTD.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8c8984-2fc0-4dd1-bbaa-4d1b459090aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 33\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences):\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_input = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}  # Move input to GPU if available\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output.cpu().numpy())\n",
    "\n",
    "    sentence_embeddings_np = np.concatenate(sentence_embeddings, axis=0)\n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "\n",
    "        # Forward pass through the classifier layer\n",
    "        logits = self.classifier(sentence_embedding)\n",
    "        \n",
    "        return logits, sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5e-06)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a955694f-88a8-457e-b14a-53a9c91d4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def euclidean_distance_loss(embeddings):\n",
    "#     n, k = embeddings.size()\n",
    "#     mean_embeddings = embeddings.mean(dim=0)\n",
    "#     distances = embeddings - mean_embeddings\n",
    "#     loss = (distances ** 2).sum(dim=1).mean() / k\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54d902b4-6d3c-4963-bfa5-9377cdda0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(embeddings):\n",
    "    n, k = embeddings.size()  # n is the batch size, k is the embedding dimension\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Calculate the mean embedding for each sample, excluding the sample itself\n",
    "    for i in range(n):\n",
    "        # Use indexing to exclude the current sample, then calculate the mean of the remaining samples\n",
    "        indices = [j for j in range(n) if j != i]\n",
    "        mean_embedding = embeddings[indices].mean(dim=0)\n",
    "        \n",
    "        # Calculate the squared Euclidean distance for the current sample\n",
    "        distance = (embeddings[i] - mean_embedding).pow(2).sum()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        loss += distance\n",
    "    \n",
    "    # Average the loss over all samples and divide by the dimension k\n",
    "    loss = loss / (n * k)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:02.079280530Z",
     "start_time": "2023-12-01T05:54:02.071418245Z"
    }
   },
   "outputs": [],
   "source": [
    "ed_loss_importance = 0.12\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:20.768410345Z",
     "start_time": "2023-12-01T05:54:06.212110157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Lower validation loss found. Model saved.\n",
      "Epoch 1/20, Training Loss: 1.20224e+00, Validation Loss: 3.65858e-01\n",
      "Epoch 2/20: Lower validation loss found. Model saved.\n",
      "Epoch 2/20, Training Loss: 2.43174e-01, Validation Loss: 1.26266e-01\n",
      "Epoch 3/20: Lower validation loss found. Model saved.\n",
      "Epoch 3/20, Training Loss: 1.16621e-01, Validation Loss: 9.74854e-02\n",
      "Epoch 4/20: Lower validation loss found. Model saved.\n",
      "Epoch 4/20, Training Loss: 8.86680e-02, Validation Loss: 8.61721e-02\n",
      "Epoch 5/20: Lower validation loss found. Model saved.\n",
      "Epoch 5/20, Training Loss: 7.55896e-02, Validation Loss: 7.92952e-02\n",
      "Epoch 6/20: Lower validation loss found. Model saved.\n",
      "Epoch 6/20, Training Loss: 6.68557e-02, Validation Loss: 7.61717e-02\n",
      "Epoch 7/20: Lower validation loss found. Model saved.\n",
      "Epoch 7/20, Training Loss: 6.02704e-02, Validation Loss: 7.18658e-02\n",
      "Epoch 8/20: Lower validation loss found. Model saved.\n",
      "Epoch 8/20, Training Loss: 5.59513e-02, Validation Loss: 6.96093e-02\n",
      "Epoch 9/20: Lower validation loss found. Model saved.\n",
      "Epoch 9/20, Training Loss: 5.22643e-02, Validation Loss: 6.77342e-02\n",
      "Epoch 10/20, Training Loss: 4.90520e-02, Validation Loss: 6.82539e-02\n",
      "Epoch 11/20: Lower validation loss found. Model saved.\n",
      "Epoch 11/20, Training Loss: 4.67675e-02, Validation Loss: 6.65711e-02\n",
      "Epoch 12/20: Lower validation loss found. Model saved.\n",
      "Epoch 12/20, Training Loss: 4.43467e-02, Validation Loss: 6.56064e-02\n",
      "Epoch 13/20, Training Loss: 4.35193e-02, Validation Loss: 6.66138e-02\n",
      "Epoch 14/20: Lower validation loss found. Model saved.\n",
      "Epoch 14/20, Training Loss: 4.12255e-02, Validation Loss: 6.25988e-02\n",
      "Epoch 15/20, Training Loss: 4.01571e-02, Validation Loss: 6.92346e-02\n",
      "Epoch 16/20, Training Loss: 3.86046e-02, Validation Loss: 6.31975e-02\n",
      "Epoch 17/20, Training Loss: 3.77048e-02, Validation Loss: 6.67920e-02\n",
      "Epoch 18/20, Training Loss: 3.67418e-02, Validation Loss: 6.34174e-02\n",
      "Epoch 19/20, Training Loss: 3.53006e-02, Validation Loss: 6.39631e-02\n",
      "Epoch 20/20, Training Loss: 3.47429e-02, Validation Loss: 6.61900e-02\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('Inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "        ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "        ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "        total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "        \n",
    "        total_loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss) \n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            total_val_loss += total_loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model\n",
    "        torch.save(model, model_name)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:29.223470776Z",
     "start_time": "2023-12-01T05:55:29.126474189Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "daf64b38465b810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:57:29.484254207Z",
     "start_time": "2023-12-01T05:57:29.442532320Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get trasformer part of the model\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:00.612845683Z",
     "start_time": "2023-12-01T05:57:30.186777749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ec7c0396-bdbf-47e5-8071-63001dea2d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:03.735123310Z",
     "start_time": "2023-12-01T05:58:03.728059325Z"
    }
   },
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cf9f9fe7-4b7f-41d7-a81d-f1cf570254eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:10.268016161Z",
     "start_time": "2023-12-01T05:58:09.663593617Z"
    }
   },
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3f854ee-b40a-452a-82b6-687e6629a9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:11.061843439Z",
     "start_time": "2023-12-01T05:58:11.036326005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ba7dac-3784-4b15-b1d0-b171099d3b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:13.108176906Z",
     "start_time": "2023-12-01T05:58:13.089972609Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "425b8398-4a6f-44bd-99a3-1d929584d5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:23.790010744Z",
     "start_time": "2023-12-01T05:58:14.102565724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875794937353319"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8cfe1c36-a25f-4b33-a787-36ec4fe62f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992555164589185"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0b5bf5ab-e718-4a7a-a710-5512372b069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:30.123817339Z",
     "start_time": "2023-12-01T05:58:29.877381627Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-12 16:24:29,033] A new study created in RDB with name: improved_ce_loss_CLINC150\n",
      "[I 2024-02-12 16:27:25,381] Trial 0 finished with value: 0.44209965141795393 and parameters: {'lr': 0.0002081506978752545, 'num_epochs': 13, 'batch_size': 256, 'ed_loss_importance': 0.4632041589223951}. Best is trial 0 with value: 0.44209965141795393.\n",
      "[I 2024-02-12 16:31:47,413] Trial 1 finished with value: 0.4712773363206093 and parameters: {'lr': 0.0002303462650008248, 'num_epochs': 20, 'batch_size': 128, 'ed_loss_importance': 0.7437696313478739}. Best is trial 1 with value: 0.4712773363206093.\n",
      "[I 2024-02-12 16:33:59,804] Trial 2 finished with value: 0.48081054148635705 and parameters: {'lr': 0.00024634902063206554, 'num_epochs': 8, 'batch_size': 128, 'ed_loss_importance': 0.8919846302133256}. Best is trial 2 with value: 0.48081054148635705.\n",
      "[I 2024-02-12 16:35:59,867] Trial 3 finished with value: 0.7579443939947929 and parameters: {'lr': 5.4207414625817564e-05, 'num_epochs': 7, 'batch_size': 128, 'ed_loss_importance': 0.798650135484202}. Best is trial 3 with value: 0.7579443939947929.\n",
      "[I 2024-02-12 16:40:55,423] Trial 4 finished with value: 0.618840547665071 and parameters: {'lr': 2.9665194679267236e-05, 'num_epochs': 15, 'batch_size': 32, 'ed_loss_importance': 0.2237092152174489}. Best is trial 3 with value: 0.7579443939947929.\n",
      "[I 2024-02-12 16:48:02,199] Trial 5 finished with value: 0.03355376466670784 and parameters: {'lr': 0.00021304894780748392, 'num_epochs': 16, 'batch_size': 16, 'ed_loss_importance': 0.9288925823770654}. Best is trial 3 with value: 0.7579443939947929.\n",
      "[I 2024-02-12 16:51:56,848] Trial 6 finished with value: 0.6862284569501317 and parameters: {'lr': 4.479404023869283e-05, 'num_epochs': 15, 'batch_size': 64, 'ed_loss_importance': 0.32548177660166133}. Best is trial 3 with value: 0.7579443939947929.\n",
      "[I 2024-02-12 16:59:24,571] Trial 7 finished with value: 0.5882924594169936 and parameters: {'lr': 3.0438856395273418e-05, 'num_epochs': 24, 'batch_size': 32, 'ed_loss_importance': 0.17756670259649454}. Best is trial 3 with value: 0.7579443939947929.\n",
      "[I 2024-02-12 17:02:30,355] Trial 8 finished with value: 0.7670848663993275 and parameters: {'lr': 1.9440196295469297e-05, 'num_epochs': 6, 'batch_size': 16, 'ed_loss_importance': 0.6091051807313976}. Best is trial 8 with value: 0.7670848663993275.\n",
      "[I 2024-02-12 17:10:14,460] Trial 9 finished with value: 0.6925003073592283 and parameters: {'lr': 9.557324370014816e-06, 'num_epochs': 25, 'batch_size': 32, 'ed_loss_importance': 0.40257359171342366}. Best is trial 8 with value: 0.7670848663993275.\n",
      "[I 2024-02-12 17:12:56,956] Trial 10 finished with value: 0.7843009341827709 and parameters: {'lr': 5.1981091613996665e-06, 'num_epochs': 5, 'batch_size': 16, 'ed_loss_importance': 0.008512254290683585}. Best is trial 10 with value: 0.7843009341827709.\n",
      "[I 2024-02-12 17:15:39,186] Trial 11 finished with value: 0.7854607654777921 and parameters: {'lr': 6.3604172566869675e-06, 'num_epochs': 5, 'batch_size': 16, 'ed_loss_importance': 0.01808180089531607}. Best is trial 11 with value: 0.7854607654777921.\n",
      "[I 2024-02-12 17:20:19,776] Trial 12 finished with value: 0.789267853599839 and parameters: {'lr': 5.240791513712217e-06, 'num_epochs': 10, 'batch_size': 16, 'ed_loss_importance': 0.007514360754310334}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:25:00,389] Trial 13 finished with value: 0.7873236948048374 and parameters: {'lr': 5.2849051227677665e-06, 'num_epochs': 10, 'batch_size': 16, 'ed_loss_importance': 0.005302264337910589}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:29:41,116] Trial 14 finished with value: 0.7437851745305417 and parameters: {'lr': 1.3051472897786952e-05, 'num_epochs': 10, 'batch_size': 16, 'ed_loss_importance': 0.11463771412865303}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:32:17,726] Trial 15 finished with value: 0.7570231728871161 and parameters: {'lr': 5.081856132305565e-06, 'num_epochs': 11, 'batch_size': 256, 'ed_loss_importance': 0.30579662701457355}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:35:07,953] Trial 16 finished with value: 0.7764249437989351 and parameters: {'lr': 1.232167071743329e-05, 'num_epochs': 10, 'batch_size': 64, 'ed_loss_importance': 0.079166924523459}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:42:58,274] Trial 17 finished with value: 0.5507012858994993 and parameters: {'lr': 9.197462190712635e-05, 'num_epochs': 18, 'batch_size': 16, 'ed_loss_importance': 0.18512693599412636}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:48:27,031] Trial 18 finished with value: 0.035153843737993545 and parameters: {'lr': 0.0008101615801589613, 'num_epochs': 12, 'batch_size': 16, 'ed_loss_importance': 0.006143258328743111}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:52:43,837] Trial 19 finished with value: 0.7824357765295895 and parameters: {'lr': 8.391015993725914e-06, 'num_epochs': 9, 'batch_size': 16, 'ed_loss_importance': 0.12794826955268818}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:55:40,936] Trial 20 finished with value: 0.789064615866178 and parameters: {'lr': 1.621338810375529e-05, 'num_epochs': 13, 'batch_size': 256, 'ed_loss_importance': 0.23345974588409468}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 17:58:38,359] Trial 21 finished with value: 0.7817923848835167 and parameters: {'lr': 8.374532509577339e-06, 'num_epochs': 13, 'batch_size': 256, 'ed_loss_importance': 0.24989002494268844}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 18:01:35,673] Trial 22 finished with value: 0.7882120764144993 and parameters: {'lr': 1.3346459326441764e-05, 'num_epochs': 13, 'batch_size': 256, 'ed_loss_importance': 0.08394337469611772}. Best is trial 12 with value: 0.789267853599839.\n",
      "[I 2024-02-12 18:04:43,307] Trial 23 finished with value: 0.7944347615238232 and parameters: {'lr': 1.7066379386953797e-05, 'num_epochs': 14, 'batch_size': 256, 'ed_loss_importance': 0.1263353472839419}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:08:23,109] Trial 24 finished with value: 0.7782235149012253 and parameters: {'lr': 2.022178797958056e-05, 'num_epochs': 17, 'batch_size': 256, 'ed_loss_importance': 0.265731920231053}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:12:22,655] Trial 25 finished with value: 0.7791459221429466 and parameters: {'lr': 1.787563751512353e-05, 'num_epochs': 19, 'batch_size': 256, 'ed_loss_importance': 0.17382349554951862}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:16:42,202] Trial 26 finished with value: 0.7715878345187825 and parameters: {'lr': 8.010676033910892e-06, 'num_epochs': 21, 'batch_size': 256, 'ed_loss_importance': 0.35701789686925933}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:19:50,242] Trial 27 finished with value: 0.7801911117762353 and parameters: {'lr': 1.0514133984567573e-05, 'num_epochs': 14, 'batch_size': 256, 'ed_loss_importance': 0.23598830944099175}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:23:06,847] Trial 28 finished with value: 0.7709327385097642 and parameters: {'lr': 1.807825256145083e-05, 'num_epochs': 12, 'batch_size': 64, 'ed_loss_importance': 0.12201852493663642}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:25:13,622] Trial 29 finished with value: 0.7699869603998523 and parameters: {'lr': 8.44043598845461e-06, 'num_epochs': 8, 'batch_size': 256, 'ed_loss_importance': 0.527458198002526}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:28:22,075] Trial 30 finished with value: 0.7845623500837758 and parameters: {'lr': 2.5667784731894805e-05, 'num_epochs': 14, 'batch_size': 256, 'ed_loss_importance': 0.07897479742018053}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:31:20,022] Trial 31 finished with value: 0.7881421925711076 and parameters: {'lr': 1.2546484953215633e-05, 'num_epochs': 13, 'batch_size': 256, 'ed_loss_importance': 0.06548944066727186}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:34:06,786] Trial 32 finished with value: 0.7931674589250801 and parameters: {'lr': 1.3938384115896447e-05, 'num_epochs': 12, 'batch_size': 256, 'ed_loss_importance': 0.1389594986475195}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:36:50,683] Trial 33 finished with value: 0.7778392435621293 and parameters: {'lr': 1.3856293561222417e-05, 'num_epochs': 11, 'batch_size': 128, 'ed_loss_importance': 0.15792606075105256}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:40:18,804] Trial 34 finished with value: 0.7810120985258271 and parameters: {'lr': 6.677702851446784e-06, 'num_epochs': 16, 'batch_size': 256, 'ed_loss_importance': 0.20663274584078678}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:43:13,950] Trial 35 finished with value: 0.7776138358203257 and parameters: {'lr': 4.062234620756497e-05, 'num_epochs': 12, 'batch_size': 128, 'ed_loss_importance': 0.2816026896289895}. Best is trial 23 with value: 0.7944347615238232.\n",
      "[I 2024-02-12 18:45:30,134] Trial 36 finished with value: 0.7967100782977937 and parameters: {'lr': 2.3869785010753777e-05, 'num_epochs': 9, 'batch_size': 256, 'ed_loss_importance': 0.13819424481199866}. Best is trial 36 with value: 0.7967100782977937.\n",
      "[I 2024-02-12 18:48:28,698] Trial 37 finished with value: 0.7510108506372976 and parameters: {'lr': 2.4387439714769836e-05, 'num_epochs': 8, 'batch_size': 32, 'ed_loss_importance': 0.14489052586403076}. Best is trial 36 with value: 0.7967100782977937.\n",
      "[I 2024-02-12 18:50:24,221] Trial 38 finished with value: 0.7947631415886438 and parameters: {'lr': 3.310365881885543e-05, 'num_epochs': 7, 'batch_size': 256, 'ed_loss_importance': 0.07920233160646306}. Best is trial 36 with value: 0.7967100782977937.\n",
      "[I 2024-02-12 18:52:19,922] Trial 39 finished with value: 0.7995277596921008 and parameters: {'lr': 3.359783057015162e-05, 'num_epochs': 7, 'batch_size': 256, 'ed_loss_importance': 0.2029847801416746}. Best is trial 39 with value: 0.7995277596921008.\n",
      "[I 2024-02-12 18:54:15,714] Trial 40 finished with value: 0.7924574854439931 and parameters: {'lr': 3.5353934665076847e-05, 'num_epochs': 7, 'batch_size': 256, 'ed_loss_importance': 0.3466137300030035}. Best is trial 39 with value: 0.7995277596921008.\n",
      "[I 2024-02-12 18:56:01,208] Trial 41 finished with value: 0.8021753367830525 and parameters: {'lr': 5.461109304892058e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.21009007096127957}. Best is trial 41 with value: 0.8021753367830525.\n",
      "[I 2024-02-12 18:57:46,666] Trial 42 finished with value: 0.806329518864634 and parameters: {'lr': 5.731907142651025e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.1960589906943562}. Best is trial 42 with value: 0.806329518864634.\n",
      "[I 2024-02-12 18:59:31,809] Trial 43 finished with value: 0.8052980492718484 and parameters: {'lr': 6.740746174742294e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.19944045292085014}. Best is trial 42 with value: 0.806329518864634.\n",
      "[I 2024-02-12 19:01:17,089] Trial 44 finished with value: 0.8090566637312359 and parameters: {'lr': 7.678054621875045e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.20447493744189094}. Best is trial 44 with value: 0.8090566637312359.\n",
      "[I 2024-02-12 19:03:16,766] Trial 45 finished with value: 0.7582471578604543 and parameters: {'lr': 6.65483010590846e-05, 'num_epochs': 6, 'batch_size': 64, 'ed_loss_importance': 0.28358027156135496}. Best is trial 44 with value: 0.8090566637312359.\n",
      "[I 2024-02-12 19:05:24,762] Trial 46 finished with value: 0.659466534281907 and parameters: {'lr': 9.108367887552036e-05, 'num_epochs': 5, 'batch_size': 32, 'ed_loss_importance': 0.2070758766115638}. Best is trial 44 with value: 0.8090566637312359.\n",
      "[I 2024-02-12 19:07:14,042] Trial 47 finished with value: 0.7820753980068377 and parameters: {'lr': 5.167592571642473e-05, 'num_epochs': 6, 'batch_size': 128, 'ed_loss_importance': 0.380581717934914}. Best is trial 44 with value: 0.8090566637312359.\n",
      "[I 2024-02-12 19:08:59,640] Trial 48 finished with value: 0.81499026170805 and parameters: {'lr': 6.0443012746444454e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.32269854273537957}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:10:44,947] Trial 49 finished with value: 0.8101650074719099 and parameters: {'lr': 7.054488720746588e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.4155801796976934}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:13:01,181] Trial 50 finished with value: 0.7842643487185016 and parameters: {'lr': 0.0001027573813034295, 'num_epochs': 9, 'batch_size': 256, 'ed_loss_importance': 0.4254900139823052}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:14:36,465] Trial 51 finished with value: 0.8149718101500268 and parameters: {'lr': 6.51768248957407e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.308216223273861}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:16:11,594] Trial 52 finished with value: 0.8068180848928387 and parameters: {'lr': 7.588728578505034e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.3252182607616465}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:17:46,572] Trial 53 finished with value: 0.7750790377099684 and parameters: {'lr': 0.00010210087683431398, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.3177146100409829}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:19:52,345] Trial 54 finished with value: 0.7873047162367266 and parameters: {'lr': 0.00012914279195150265, 'num_epochs': 8, 'batch_size': 256, 'ed_loss_importance': 0.30764334092315393}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:22:00,678] Trial 55 finished with value: 0.760876226498378 and parameters: {'lr': 4.468078776452156e-05, 'num_epochs': 5, 'batch_size': 32, 'ed_loss_importance': 0.43072775226782084}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:23:46,152] Trial 56 finished with value: 0.8041928691930225 and parameters: {'lr': 7.591464956398533e-05, 'num_epochs': 6, 'batch_size': 256, 'ed_loss_importance': 0.3466861340197876}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:25:59,295] Trial 57 finished with value: 0.6783629322060072 and parameters: {'lr': 5.692051595172797e-05, 'num_epochs': 7, 'batch_size': 64, 'ed_loss_importance': 0.38618456861440587}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:28:05,494] Trial 58 finished with value: 0.7872968118009073 and parameters: {'lr': 0.00012388844382589905, 'num_epochs': 8, 'batch_size': 256, 'ed_loss_importance': 0.462665781963459}. Best is trial 48 with value: 0.81499026170805.\n",
      "[I 2024-02-12 19:29:40,937] Trial 59 finished with value: 0.8152653456182472 and parameters: {'lr': 7.991476058665343e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.278269861552616}. Best is trial 59 with value: 0.8152653456182472.\n",
      "[I 2024-02-12 19:31:16,300] Trial 60 finished with value: 0.802606953538755 and parameters: {'lr': 7.975922056913486e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.260223477439164}. Best is trial 59 with value: 0.8152653456182472.\n",
      "[I 2024-02-12 19:32:51,323] Trial 61 finished with value: 0.8221863266198032 and parameters: {'lr': 5.697280720639533e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.2967498392214456}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:34:26,835] Trial 62 finished with value: 0.8161041274424016 and parameters: {'lr': 6.756256664003902e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.29914922580059156}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:36:23,410] Trial 63 finished with value: 0.7971902128699537 and parameters: {'lr': 4.142089993430113e-05, 'num_epochs': 7, 'batch_size': 256, 'ed_loss_importance': 0.25185869495159374}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:37:58,914] Trial 64 finished with value: 0.8197395312945168 and parameters: {'lr': 6.497960268945596e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.29034360887278626}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:39:34,436] Trial 65 finished with value: 0.8132984647595355 and parameters: {'lr': 5.1103671872653496e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.2946572157533348}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:44:04,705] Trial 66 finished with value: 0.765605630226837 and parameters: {'lr': 4.6879100025782067e-05, 'num_epochs': 22, 'batch_size': 256, 'ed_loss_importance': 0.28721144909585755}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:45:43,671] Trial 67 finished with value: 0.765568381270714 and parameters: {'lr': 5.675629133005134e-05, 'num_epochs': 5, 'batch_size': 128, 'ed_loss_importance': 0.36001262852765864}. Best is trial 61 with value: 0.8221863266198032.\n",
      "[I 2024-02-12 19:47:18,660] Trial 68 finished with value: 0.809009057472111 and parameters: {'lr': 3.936169894330667e-05, 'num_epochs': 5, 'batch_size': 256, 'ed_loss_importance': 0.3025145671459058}. Best is trial 61 with value: 0.8221863266198032.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Assuming other necessary imports are already there\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    seed_value=42\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    lr = trial.suggest_float('lr', 5e-6,1e-3, log=True)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 5, 25)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "    ed_loss_importance = trial.suggest_float('ed_loss_importance', 0, 1)\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # Model setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "    model = TextClassifier(transformer_model, len(unique_intents))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            \n",
    "            total_loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += total_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss) \n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "                ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "                ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "                total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "                total_val_loss += total_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        validation_losses.append(avg_train_loss)\n",
    "    trial.set_user_attr(\"training_losses\", training_losses)\n",
    "    trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "    fine_transformer = model.transformer\n",
    "    train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "    val_embeddings = encode_sentences(fine_transformer, val_sentences)\n",
    "    oos_val_embeddings = encode_sentences(fine_transformer, oos_val_sentences)\n",
    "\n",
    "    intent_means = {}\n",
    "    for encoded_label in np.unique(encoded_train_labels):\n",
    "        # Find indices where the encoded label matches\n",
    "        indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "        # Calculate the mean embedding for the current intent\n",
    "        intent_embeddings = train_embeddings[indices]\n",
    "        intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "        # Use the encoded label as the dictionary key\n",
    "        intent_means[encoded_label] = intent_mean\n",
    "    covariance = np.cov(train_embeddings, rowvar=False)\n",
    "    cov_inverse = inv(covariance)\n",
    "    val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "    oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "    # True binary labels: 0 for in-domain and 1 for OOD\n",
    "    y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "    # Combine the scores\n",
    "    y_scores = val_scores + oos_val_scores\n",
    "\n",
    "    # Compute AUPR\n",
    "    aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    return aupr\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize',  study_name='improved_ce_loss_CLINC150', storage='sqlite:///desperate.db')\n",
    "study.optimize(objective, n_trials=1000)  # n_trials is the number of iterations\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcac44a-52a1-4f17-8544-bee6e9c5d984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
