{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:46:57.637647142Z",
     "start_time": "2023-12-01T05:46:56.101609820Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40499132-518f-4f48-a389-f5a281434e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value=10912\n",
    "# random.seed(seed_value)\n",
    "# np.random.seed(seed_value)\n",
    "# torch.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693267d-9a81-4282-b9bb-d853102347c5",
   "metadata": {},
   "source": [
    "# Load CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:06.596136455Z",
     "start_time": "2023-12-01T05:47:06.586541155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "# Extracting data\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']\n",
    "\n",
    "oos_train_data = data['oos_train']\n",
    "oos_val_data = data['oos_val']\n",
    "oos_test_data = data['oos_test']\n",
    "\n",
    "# Get sentences and labels\n",
    "train_sentences = [item[0] for item in train_data]\n",
    "train_labels = [item[1] for item in train_data]\n",
    "\n",
    "val_sentences = [item[0] for item in val_data]\n",
    "val_labels = [item[1] for item in val_data]\n",
    "\n",
    "test_sentences = [item[0] for item in test_data]\n",
    "test_labels = [item[1] for item in test_data]\n",
    "\n",
    "oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "model_name = \"improved_ce_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54766b2c-cd88-4636-bff0-ba15c4272f11",
   "metadata": {},
   "source": [
    "# Load SLURP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a73b50-057e-4813-8606-bed9789c138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_path):\n",
    "#     sentences = []\n",
    "#     scenarios = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             sentence = data.get('sentence', None)\n",
    "#             scenario = data.get('scenario', None)\n",
    "#             if sentence is not None and scenario is not None:\n",
    "#                 sentences.append(sentence)\n",
    "#                 scenarios.append(scenario)\n",
    "#     return sentences, scenarios\n",
    "\n",
    "# # Randomly select one domain to be out of scope\n",
    "# unique_scenarios = {'alarm', 'audio', 'calendar', 'cooking', 'datetime', 'email', 'general', 'iot', 'lists', 'music', 'news', 'play', 'qa', 'recommendation', 'social', 'takeaway', 'transport', 'weather'}\n",
    "# oos_scenario = random.choice(list(unique_scenarios))\n",
    "\n",
    "# # Load data from files\n",
    "# train_sentences, train_labels = load_data('slurp/dataset/slurp/train.jsonl')\n",
    "# val_sentences, val_labels = load_data('slurp/dataset/slurp/devel.jsonl')\n",
    "# test_sentences, test_labels = load_data('slurp/dataset/slurp/test.jsonl')\n",
    "\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "# model_name = \"improved_ce_model_bert_SLURP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c9cf7-222d-4195-ae55-1fee13ab45b5",
   "metadata": {},
   "source": [
    "# Load Banking77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6c0847-fc1c-4964-96a7-7ffea08c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test',\n",
    "#     'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "#     'oos_test': f'{base_dir}/ood-oos/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "# oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "# oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "# model_name = \"improved_ce_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885f3de-13b4-4e6f-900f-5ed5947ea7da",
   "metadata": {},
   "source": [
    "# SNIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc0dbd1-2cfe-45a0-9912-6c61d111b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/SNIPS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "# unique_scenarios = set(train_labels)\n",
    "# # oos_scenario = random.choice(list(unique_scenarios))\n",
    "# oos_scenario = 'AddToPlaylist'\n",
    "# # Separate out of scope data\n",
    "# oos_train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l == oos_scenario]\n",
    "# oos_val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l == oos_scenario]\n",
    "# oos_test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l == oos_scenario]\n",
    "\n",
    "# # Remove out of scope data from original sets\n",
    "# train_data = [(s, l) for s, l in zip(train_sentences, train_labels) if l != oos_scenario]\n",
    "# val_data = [(s, l) for s, l in zip(val_sentences, val_labels) if l != oos_scenario]\n",
    "# test_data = [(s, l) for s, l in zip(test_sentences, test_labels) if l != oos_scenario]\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# oos_scenario\n",
    "\n",
    "# model_name = \"improved_ce_model_bert_SNIP.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fd985-bb3f-4935-bc1a-6dbf39f3b175",
   "metadata": {},
   "source": [
    "# ROSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e50a8ee-c7d3-40f0-b7e9-0de11a40435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"cmaldona/Generalization-MultiClass-CLINC150-ROSTD\", \"rostd+\")\n",
    "\n",
    "# train_sentences = []\n",
    "# train_labels = []\n",
    "# val_sentences = []\n",
    "# val_labels = []\n",
    "# test_sentences = []\n",
    "# test_labels = []\n",
    "# oos_test_sentences = []\n",
    "\n",
    "# # Extract training data\n",
    "# for example in dataset['train']:\n",
    "#     train_sentences.append(example['data'].lower())\n",
    "#     train_labels.append(example['labels'])\n",
    "\n",
    "# # Extract validation data\n",
    "# for example in dataset['validation']:\n",
    "#     val_sentences.append(example['data'].lower())\n",
    "#     val_labels.append(example['labels'])\n",
    "\n",
    "# # Extract test data and separate ID from OOS\n",
    "# for example in dataset['test']:\n",
    "#     if example['generalisation'] == 'ID':\n",
    "#         test_sentences.append(example['data'].lower())\n",
    "#         test_labels.append(example['labels'])\n",
    "#     elif example['generalisation'] == 'near-OOD' or example['generalisation'] == 'far-OOD':# OOS\n",
    "#         try:\n",
    "#             oos_test_sentences.append(example['data'].lower())\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "\n",
    "# model_name = \"improved_ce_model_bert_ROSTD.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:11.994721178Z",
     "start_time": "2023-12-01T05:47:11.979633066Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:18.554957176Z",
     "start_time": "2023-12-01T05:47:17.175180209Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af8c8984-2fc0-4dd1-bbaa-4d1b459090aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 33\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:37.721316035Z",
     "start_time": "2023-12-01T05:48:37.670663494Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:48:43.266219171Z",
     "start_time": "2023-12-01T05:48:42.916645883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = transformer_output.last_hidden_state.max(dim=1).values\n",
    "\n",
    "        # Forward pass through the classifier layer\n",
    "        logits = self.classifier(sentence_embedding)\n",
    "        \n",
    "        return logits, sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:01.432727970Z",
     "start_time": "2023-12-01T05:54:00.858905115Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5.2849051227677665e-06)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a955694f-88a8-457e-b14a-53a9c91d4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def euclidean_distance_loss(embeddings):\n",
    "#     n, k = embeddings.size()\n",
    "#     mean_embeddings = embeddings.mean(dim=0)\n",
    "#     distances = embeddings - mean_embeddings\n",
    "#     loss = (distances ** 2).sum(dim=1).mean() / k\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54d902b4-6d3c-4963-bfa5-9377cdda0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(embeddings):\n",
    "    n, k = embeddings.size()  # n is the batch size, k is the embedding dimension\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Calculate the mean embedding for each sample, excluding the sample itself\n",
    "    for i in range(n):\n",
    "        # Use indexing to exclude the current sample, then calculate the mean of the remaining samples\n",
    "        indices = [j for j in range(n) if j != i]\n",
    "        mean_embedding = embeddings[indices].mean(dim=0)\n",
    "        \n",
    "        # Calculate the squared Euclidean distance for the current sample\n",
    "        distance = (embeddings[i] - mean_embedding).pow(2).sum()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        loss += distance\n",
    "    \n",
    "    # Average the loss over all samples and divide by the dimension k\n",
    "    loss = loss / (n * k)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:54:02.079280530Z",
     "start_time": "2023-12-01T05:54:02.071418245Z"
    }
   },
   "outputs": [],
   "source": [
    "ed_loss_importance = 0.005302264337910589\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:20.768410345Z",
     "start_time": "2023-12-01T05:54:06.212110157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: Lower validation loss found. Model saved.\n",
      "Epoch 1/25, Training Loss: 4.30027e+00, Validation Loss: 2.74053e+00\n",
      "Epoch 2/25: Lower validation loss found. Model saved.\n",
      "Epoch 2/25, Training Loss: 1.99638e+00, Validation Loss: 1.03744e+00\n",
      "Epoch 3/25: Lower validation loss found. Model saved.\n",
      "Epoch 3/25, Training Loss: 8.31367e-01, Validation Loss: 5.03771e-01\n",
      "Epoch 4/25: Lower validation loss found. Model saved.\n",
      "Epoch 4/25, Training Loss: 4.09957e-01, Validation Loss: 3.20754e-01\n",
      "Epoch 5/25: Lower validation loss found. Model saved.\n",
      "Epoch 5/25, Training Loss: 2.31663e-01, Validation Loss: 2.44694e-01\n",
      "Epoch 6/25: Lower validation loss found. Model saved.\n",
      "Epoch 6/25, Training Loss: 1.43758e-01, Validation Loss: 2.09301e-01\n",
      "Epoch 7/25: Lower validation loss found. Model saved.\n",
      "Epoch 7/25, Training Loss: 9.22048e-02, Validation Loss: 1.89012e-01\n",
      "Epoch 8/25: Lower validation loss found. Model saved.\n",
      "Epoch 8/25, Training Loss: 6.25512e-02, Validation Loss: 1.85725e-01\n",
      "Epoch 9/25: Lower validation loss found. Model saved.\n",
      "Epoch 9/25, Training Loss: 4.46348e-02, Validation Loss: 1.77358e-01\n",
      "Epoch 10/25, Training Loss: 3.27650e-02, Validation Loss: 1.81256e-01\n",
      "Epoch 11/25: Lower validation loss found. Model saved.\n",
      "Epoch 11/25, Training Loss: 2.63939e-02, Validation Loss: 1.72795e-01\n",
      "Epoch 12/25, Training Loss: 1.95248e-02, Validation Loss: 1.75433e-01\n",
      "Epoch 13/25: Lower validation loss found. Model saved.\n",
      "Epoch 13/25, Training Loss: 1.67204e-02, Validation Loss: 1.70552e-01\n",
      "Epoch 14/25: Lower validation loss found. Model saved.\n",
      "Epoch 14/25, Training Loss: 1.43899e-02, Validation Loss: 1.70084e-01\n",
      "Epoch 15/25, Training Loss: 1.29859e-02, Validation Loss: 1.82044e-01\n",
      "Epoch 16/25, Training Loss: 9.06959e-03, Validation Loss: 1.82917e-01\n",
      "Epoch 17/25, Training Loss: 7.69462e-03, Validation Loss: 1.81120e-01\n",
      "Epoch 18/25, Training Loss: 8.81216e-03, Validation Loss: 1.94889e-01\n",
      "Epoch 19/25, Training Loss: 7.08602e-03, Validation Loss: 1.88544e-01\n",
      "Epoch 20/25, Training Loss: 5.92858e-03, Validation Loss: 1.90549e-01\n",
      "Epoch 21/25, Training Loss: 4.73646e-03, Validation Loss: 2.00034e-01\n",
      "Epoch 22/25, Training Loss: 4.45833e-03, Validation Loss: 2.03355e-01\n",
      "Epoch 23/25, Training Loss: 3.89686e-03, Validation Loss: 2.37486e-01\n",
      "Epoch 24/25, Training Loss: 7.71138e-03, Validation Loss: 1.85648e-01\n",
      "Epoch 25/25, Training Loss: 8.23076e-03, Validation Loss: 1.93579e-01\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('Inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "        ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "        ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "        total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "        \n",
    "        total_loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss) \n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            total_val_loss += total_loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model\n",
    "        torch.save(model, model_name)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:29.223470776Z",
     "start_time": "2023-12-01T05:55:29.126474189Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "daf64b38465b810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:57:29.484254207Z",
     "start_time": "2023-12-01T05:57:29.442532320Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get trasformer part of the model\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:00.612845683Z",
     "start_time": "2023-12-01T05:57:30.186777749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "val_embeddings = encode_sentences(fine_model, val_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_val_embeddings = encode_sentences(fine_model, oos_val_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec7c0396-bdbf-47e5-8071-63001dea2d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:03.735123310Z",
     "start_time": "2023-12-01T05:58:03.728059325Z"
    }
   },
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf9f9fe7-4b7f-41d7-a81d-f1cf570254eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:10.268016161Z",
     "start_time": "2023-12-01T05:58:09.663593617Z"
    }
   },
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3f854ee-b40a-452a-82b6-687e6629a9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:11.061843439Z",
     "start_time": "2023-12-01T05:58:11.036326005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27ba7dac-3784-4b15-b1d0-b171099d3b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:13.108176906Z",
     "start_time": "2023-12-01T05:58:13.089972609Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72718501-4f91-42cf-9fd8-8b0de96cd618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7604563358162935"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = val_scores + oos_val_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "425b8398-4a6f-44bd-99a3-1d929584d5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:23.790010744Z",
     "start_time": "2023-12-01T05:58:14.102565724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162836733999012"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cfe1c36-a25f-4b33-a787-36ec4fe62f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768182222222221"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0b5bf5ab-e718-4a7a-a710-5512372b069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:30.123817339Z",
     "start_time": "2023-12-01T05:58:29.877381627Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10710f00-bedb-4060-9cfc-d3a2814b766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-15 17:49:02,599] A new study created in RDB with name: improved_ce_loss_CLINC150_min_loss2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: Lower validation loss found. Model saved.\n",
      "Epoch 2/25: Lower validation loss found. Model saved.\n",
      "Epoch 3/25: Lower validation loss found. Model saved.\n",
      "Epoch 4/25: Lower validation loss found. Model saved.\n",
      "Epoch 5/25: Lower validation loss found. Model saved.\n",
      "Epoch 6/25: Lower validation loss found. Model saved.\n",
      "Epoch 7/25: Lower validation loss found. Model saved.\n",
      "Epoch 8/25: Lower validation loss found. Model saved.\n",
      "Epoch 9/25: Lower validation loss found. Model saved.\n",
      "Epoch 10/25: Lower validation loss found. Model saved.\n",
      "Epoch 11/25: Lower validation loss found. Model saved.\n",
      "Epoch 12/25: Lower validation loss found. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-02-15 17:51:08,149] Trial 0 failed with parameters: {'lr': 5e-06, 'batch_size': 256} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ztybigcat/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5032/517874654.py\", line 47, in objective\n",
      "    total_train_loss += total_loss.item()\n",
      "                        ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-15 17:51:08,149] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 115\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# fine_transformer = model.transformer\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# train_embeddings = encode_sentences(fine_transformer, train_sentences)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# val_embeddings = encode_sentences(fine_transformer, val_sentences)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[1;32m    114\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,  study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimproved_ce_loss_CLINC150_min_loss2\u001b[39m\u001b[38;5;124m'\u001b[39m, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///desperate.db\u001b[39m\u001b[38;5;124m'\u001b[39m, load_if_exists\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# n_trials is the number of iterations\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Get the best parameters\u001b[39;00m\n\u001b[1;32m    118\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Desktop/my_own_ood/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[55], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     44\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     50\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    writer = SummaryWriter()\n",
    "    seed_value=42\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    lr = trial.suggest_categorical('lr', [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3])\n",
    "    num_epochs = 25\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "    # ed_loss_importance = trial.suggest_float('ed_loss_importance', 0.05, 0.2)\n",
    "    ed_loss_importance = 0.1\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # Model setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "    model = TextClassifier(transformer_model, len(unique_intents))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "            ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "            ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "            total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "            \n",
    "            total_loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += total_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_ed_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                predictions, embeddings = model(input_ids, attention_mask)  # Forward pass\n",
    "                ce_loss = loss_function(predictions, labels)  # Cross-Entropy loss\n",
    "                ed_loss = euclidean_distance_loss(embeddings)  # Euclidean distance loss\n",
    "                total_loss = ce_loss + ed_loss_importance * ed_loss  # Combine the losses\n",
    "                total_val_loss += total_loss.item()\n",
    "                total_ce_loss += ce_loss.item()\n",
    "                total_ed_loss += ed_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(val_dataloader)\n",
    "        avg_ed_loss = total_ed_loss / len(val_dataloader)\n",
    "        writer.add_scalar(\"Validation/Average CE Loss\", avg_ce_loss, epoch)\n",
    "        writer.add_scalar(\"Validation/Average ED Loss\", avg_ed_loss, epoch)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            #torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "    trial.set_user_attr(\"training_losses\", training_losses)\n",
    "    trial.set_user_attr(\"validation_losses\", validation_losses)\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "    # fine_transformer = model.transformer\n",
    "    # train_embeddings = encode_sentences(fine_transformer, train_sentences)\n",
    "    # val_embeddings = encode_sentences(fine_transformer, val_sentences)\n",
    "    # oos_val_embeddings = encode_sentences(fine_transformer, oos_val_sentences)\n",
    "\n",
    "    # intent_means = {}\n",
    "    # for encoded_label in np.unique(encoded_train_labels):\n",
    "    #     # Find indices where the encoded label matches\n",
    "    #     indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "        \n",
    "    #     # Calculate the mean embedding for the current intent\n",
    "    #     intent_embeddings = train_embeddings[indices]\n",
    "    #     intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "        \n",
    "    #     # Use the encoded label as the dictionary key\n",
    "    #     intent_means[encoded_label] = intent_mean\n",
    "    # covariance = np.cov(train_embeddings, rowvar=False)\n",
    "    # cov_inverse = inv(covariance)\n",
    "    # val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "    # oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "    # # True binary labels: 0 for in-domain and 1 for OOD\n",
    "    # y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "    # # Combine the scores\n",
    "    # y_scores = val_scores + oos_val_scores\n",
    "\n",
    "    # # Compute AUPR\n",
    "    # aupr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    # return aupr\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize',  study_name='improved_ce_loss_CLINC150_min_loss_feb15', storage='sqlite:///desperate.db', load_if_exists= True)\n",
    "study.optimize(objective, n_trials=1000)  # n_trials is the number of iterations\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcac44a-52a1-4f17-8544-bee6e9c5d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.load_study(study_name='improved_ce_loss_CLINC150', storage='sqlite:///desperate.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f368fea-dd7c-4934-9851-01be1fc63c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = sorted(\n",
    "    study.trials, \n",
    "    key=lambda trial: min(trial.user_attrs.get('validation_losses', [float('inf')]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599cd4c-86f0-44ba-ab9f-f5e7d88cb180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
