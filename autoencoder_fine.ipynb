{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:04.721661140Z",
     "start_time": "2023-12-01T07:38:03.867605995Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6496a9a-49d4-4e85-8721-5cb1f0cf4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=883070\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb43cd-459c-427b-8d24-933b40277cf5",
   "metadata": {},
   "source": [
    "# Cerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cdb473e-9072-4825-b0c5-a730d3b26e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_suffix = \"just_talk_embedded_reject_twitter_blind-testonly.en-US.NCS5.1.final_1.input\"\n",
    "def load_sentences_and_labels(data_dir, subset, in_system=True, suffix=\".input\"):\n",
    "    \"\"\"\n",
    "    Load sentences and their labels from a given directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Base directory containing the data subsets ('dev', 'test', 'train').\n",
    "    - subset: Subset of data to load ('dev', 'train', 'test').\n",
    "    - in_system: Boolean indicating whether to load InSys (True) or OOS (False) data.\n",
    "    - suffix: File suffix to look for.\n",
    "    \n",
    "    Returns:\n",
    "    - sentences: List of sentences loaded from the files.\n",
    "    - labels: List of corresponding labels derived from file names.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    subset_dir = \"InSys\" if in_system else \"OOS\"\n",
    "    full_path = os.path.join(data_dir, subset, subset_dir)\n",
    "    \n",
    "    for filename in os.listdir(full_path):\n",
    "        if filename.endswith(suffix):\n",
    "            label = filename.replace(suffix, '')\n",
    "            with open(os.path.join(full_path, filename), 'r', encoding='utf-8') as file:\n",
    "                file_sentences = file.readlines()\n",
    "                sentences.extend([sentence.strip() for sentence in file_sentences])\n",
    "                labels.extend([label] * len(file_sentences))\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Specify the base directory where your 'dev', 'test', 'train' directories are located\n",
    "base_dir = 'cerence_data'\n",
    "\n",
    "# Load the datasets\n",
    "train_sentences, train_labels = load_sentences_and_labels(base_dir, 'train', suffix=\".train.input\")\n",
    "val_sentences, val_labels = load_sentences_and_labels(base_dir, 'dev', suffix=\".dev.input\")\n",
    "test_sentences, test_labels = load_sentences_and_labels(base_dir, 'test', in_system=True, suffix=\".test.input\")\n",
    "oos_test_sentences, _ = load_sentences_and_labels(base_dir, 'test', in_system=False, suffix=ood_suffix)\n",
    "\n",
    "# Split half as val\n",
    "random.shuffle(oos_test_sentences)\n",
    "split_index = len(oos_test_sentences) // 2\n",
    "\n",
    "# Split the sentences into two halves\n",
    "oos_val_sentences = oos_test_sentences[:split_index]\n",
    "oos_test_sentences = oos_test_sentences[split_index:]\n",
    "\n",
    "train_sentences, _, train_labels, _ = train_test_split(\n",
    "    train_sentences, train_labels, train_size=1/25, stratify=train_labels, random_state=seed_value)\n",
    "test_sentences, _, test_labels, _ = train_test_split(\n",
    "    test_sentences, test_labels, train_size=1/20, stratify=test_labels, random_state=seed_value)\n",
    "\n",
    "model_name = \"ae_model_bert_cerence.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2666d-25dd-4595-9681-21c46df0c10d",
   "metadata": {},
   "source": [
    "# SNIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dca5064b-a953-4d35-8bde-72e87ba61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/SNIPS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test'\n",
    "# }\n",
    "# # Define paths to the dataset directories\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# def extract_sentences_labels(dataset):\n",
    "#     sentences = [e.text for e in dataset]\n",
    "#     labels = [e.label for e in dataset]\n",
    "#     return sentences, labels\n",
    "\n",
    "# all_train_sentences, all_train_labels = extract_sentences_labels(datasets['train'])\n",
    "# all_val_sentences, all_val_labels = extract_sentences_labels(datasets['valid'])\n",
    "# all_test_sentences, all_test_labels = extract_sentences_labels(datasets['test'])\n",
    "\n",
    "# # Count the frequency of each label in the training data\n",
    "# label_freq = collections.Counter(all_train_labels)\n",
    "\n",
    "# # Calculate the threshold for selecting 75% of the data as in-domain\n",
    "# threshold = len(all_train_labels) * 0.75\n",
    "# cumulative_count = 0\n",
    "# selected_labels = []\n",
    "\n",
    "# # Randomize the label order for fairness\n",
    "# all_labels = list(label_freq.keys())\n",
    "# random.shuffle(all_labels)\n",
    "\n",
    "# for label in all_labels:\n",
    "#     count = label_freq[label]\n",
    "#     cumulative_count += count\n",
    "#     selected_labels.append(label)\n",
    "#     if cumulative_count >= threshold:\n",
    "#         break\n",
    "\n",
    "# # Function to filter data by selected labels\n",
    "# def filter_data(sentences, labels, selected_labels):\n",
    "#     filtered_data = [(s, l) for s, l in zip(sentences, labels) if l in selected_labels]\n",
    "#     return [item[0] for item in filtered_data], [item[1] for item in filtered_data]\n",
    "\n",
    "# # Filter the datasets\n",
    "# train_sentences, train_labels = filter_data(all_train_sentences, all_train_labels, selected_labels)\n",
    "# val_sentences, val_labels = filter_data(all_val_sentences, all_val_labels, selected_labels)\n",
    "# test_sentences, test_labels = filter_data(all_test_sentences, all_test_labels, selected_labels)\n",
    "\n",
    "# # Identify and separate the out-of-domain data\n",
    "# oos_labels = set(all_labels) - set(selected_labels)\n",
    "# oos_train_sentences, oos_train_labels = filter_data(all_train_sentences, all_train_labels, oos_labels)\n",
    "# oos_val_sentences, oos_val_labels = filter_data(all_val_sentences, all_val_labels, oos_labels)\n",
    "# oos_test_sentences, oos_test_labels = filter_data(all_test_sentences, all_test_labels, oos_labels)\n",
    "\n",
    "# # Evenly split oos_train data between validation and test\n",
    "# half_index = len(oos_train_sentences) // 2\n",
    "# oos_val_sentences.extend(oos_train_sentences[:half_index])\n",
    "# oos_val_labels.extend(oos_train_labels[:half_index])\n",
    "# oos_test_sentences.extend(oos_train_sentences[half_index:])\n",
    "# oos_test_labels.extend(oos_train_labels[half_index:])\n",
    "# model_name = \"ae_model_bert_SNIP.pth\"\n",
    "# # oos_scenario is no longer a single scenario but a list of OOD labels\n",
    "# oos_scenarios = list(oos_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9e8e3-cc26-4d67-87a9-50ddc49a4fa3",
   "metadata": {},
   "source": [
    "# StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e6b8204-027c-4956-85ed-c03f8f2c3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_number = 1  # Adjust this to the split number you're interested in\n",
    "\n",
    "# # Define the base directory where the splits are stored\n",
    "# base_dir = 'stackoverflow_data'  # Update this with your actual directory path\n",
    "\n",
    "# # Construct the path to the specific split\n",
    "# split_dir = os.path.join(base_dir, f'split{split_number}')\n",
    "\n",
    "# # Function to load data from a .pkl file\n",
    "# def load_data_from_split(split_dir, file_name):\n",
    "#     with open(os.path.join(split_dir, file_name), 'rb') as file:\n",
    "#         return pickle.load(file)\n",
    "\n",
    "# # Load the datasets\n",
    "# train_sentences = load_data_from_split(split_dir, 'train_sentences.pkl')\n",
    "# train_labels = load_data_from_split(split_dir, 'train_labels.pkl')\n",
    "# val_sentences = load_data_from_split(split_dir, 'val_sentences.pkl')\n",
    "# val_labels = load_data_from_split(split_dir, 'val_labels.pkl')\n",
    "# test_sentences = load_data_from_split(split_dir, 'test_sentences.pkl')\n",
    "# test_labels = load_data_from_split(split_dir, 'test_labels.pkl')\n",
    "# oos_val_sentences = load_data_from_split(split_dir, 'oos_val_sentences.pkl')\n",
    "# oos_test_sentences = load_data_from_split(split_dir, 'oos_test_sentences.pkl')\n",
    "# model_name = \"ae_model_bert_Stack.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f96c05-0753-42a0-a801-beb4c2aed962",
   "metadata": {},
   "source": [
    "# CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:04.774005458Z",
     "start_time": "2023-12-01T07:38:04.723166259Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load the dataset\n",
    "# with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "#     data = json.load(file)\n",
    "# # Extracting data\n",
    "# train_data = data['train']\n",
    "# val_data = data['val']\n",
    "# test_data = data['test']\n",
    "\n",
    "# oos_train_data = data['oos_train']\n",
    "# oos_val_data = data['oos_val']\n",
    "# oos_test_data = data['oos_test']\n",
    "\n",
    "# # Get sentences and labels\n",
    "# train_sentences = [item[0] for item in train_data]\n",
    "# train_labels = [item[1] for item in train_data]\n",
    "\n",
    "# val_sentences = [item[0] for item in val_data]\n",
    "# val_labels = [item[1] for item in val_data]\n",
    "\n",
    "# test_sentences = [item[0] for item in test_data]\n",
    "# test_labels = [item[1] for item in test_data]\n",
    "\n",
    "# oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "# oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "# oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "# model_name = \"ae_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b779be-a7df-4697-ae69-bba69171ac2c",
   "metadata": {},
   "source": [
    "# BANKING77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3b414e9-6912-4c60-95f4-7abd2a42d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the IntentExample class and load_intent_examples function as provided\n",
    "# class IntentExample:\n",
    "#     def __init__(self, text, label, do_lower_case):\n",
    "#         self.original_text = text\n",
    "#         self.text = text\n",
    "#         self.label = label\n",
    "#         if do_lower_case:\n",
    "#             self.text = self.text.lower()\n",
    "\n",
    "# def load_intent_examples(file_path, do_lower_case=True):\n",
    "#     examples = []\n",
    "#     with open(f'{file_path}/seq.in', 'r', encoding=\"utf-8\") as f_text, open(f'{file_path}/label', 'r', encoding=\"utf-8\") as f_label:\n",
    "#         for text, label in zip(f_text, f_label):\n",
    "#             e = IntentExample(text.strip(), label.strip(), do_lower_case)\n",
    "#             examples.append(e)\n",
    "#     return examples\n",
    "\n",
    "# # Define paths to the dataset directories\n",
    "# base_dir = 'Few-Shot-Intent-Detection/Datasets/BANKING77-OOS'\n",
    "# paths = {\n",
    "#     'train': f'{base_dir}/train',\n",
    "#     'valid': f'{base_dir}/valid',\n",
    "#     'test': f'{base_dir}/test',\n",
    "#     'oos_val': f'{base_dir}/ood-oos/valid',\n",
    "#     'oos_test': f'{base_dir}/ood-oos/test'\n",
    "# }\n",
    "# datasets = {key: load_intent_examples(path) for key, path in paths.items()}\n",
    "\n",
    "# # Extract sentences and labels from the loaded datasets\n",
    "# train_sentences = [e.text for e in datasets['train']]\n",
    "# train_labels = [e.label for e in datasets['train']]\n",
    "\n",
    "# val_sentences = [e.text for e in datasets['valid']]\n",
    "# val_labels = [e.label for e in datasets['valid']]\n",
    "\n",
    "# test_sentences = [e.text for e in datasets['test']]\n",
    "# test_labels = [e.label for e in datasets['test']]\n",
    "\n",
    "# oos_val_sentences = [e.text for e in datasets['oos_val']]\n",
    "# oos_test_sentences = [e.text for e in datasets['oos_test']]\n",
    "# model_name = \"ae_model_bert_BANKING77.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f002ca-fc78-4c2f-b171-e10e46642af3",
   "metadata": {},
   "source": [
    "# ROSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6980cc66-ba8d-492c-881e-7b7406e7785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"cmaldona/Generalization-MultiClass-CLINC150-ROSTD\", \"rostd+\")\n",
    "\n",
    "# train_sentences = []\n",
    "# train_labels = []\n",
    "# val_sentences = []\n",
    "# val_labels = []\n",
    "# test_sentences = []\n",
    "# test_labels = []\n",
    "# oos_test_sentences = []\n",
    "\n",
    "# # Extract training data\n",
    "# for example in dataset['train']:\n",
    "#     train_sentences.append(example['data'].lower())\n",
    "#     train_labels.append(example['labels'])\n",
    "\n",
    "# # Extract validation data\n",
    "# for example in dataset['validation']:\n",
    "#     val_sentences.append(example['data'].lower())\n",
    "#     val_labels.append(example['labels'])\n",
    "\n",
    "# # Extract test data and separate ID from OOS\n",
    "# for example in dataset['test']:\n",
    "#     if example['generalisation'] == 'ID':\n",
    "#         test_sentences.append(example['data'].lower())\n",
    "#         test_labels.append(example['labels'])\n",
    "#     elif example['generalisation'] == 'near-OOD' or example['generalisation'] == 'far-OOD':# OOS\n",
    "#         try:\n",
    "#             oos_test_sentences.append(example['data'].lower())\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "\n",
    "# model_name = \"ae_model_bert_ROSTD.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22ba83dc-95ed-4e5a-9484-ce28204ec991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{seed_value}_{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:05.423215976Z",
     "start_time": "2023-12-01T07:38:05.406651396Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)\n",
    "encoded_test_labels = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:07.327824427Z",
     "start_time": "2023-12-01T07:38:06.135883081Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ab67fd0-7073-4495-b464-8ecb4b2b3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 96\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9090dc4-fe28-4c23-b21b-5ba229eb8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)\n",
    "test_dataset = TextDataset(test_sentences, encoded_test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a8213f6-4be9-41e6-95cd-07d9fa53a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:07.931914441Z",
     "start_time": "2023-12-01T07:38:07.768420485Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:09.600227529Z",
     "start_time": "2023-12-01T07:38:09.154881377Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder1 = nn.Linear(768, 512)\n",
    "        self.encoder2 = nn.Linear(512, 64)\n",
    "        self.encoder3 = nn.Linear(64, 16)\n",
    "        # Decoder layers\n",
    "        self.decoder1 = nn.Linear(16, 64)\n",
    "        self.decoder2 = nn.Linear(64, 512)\n",
    "        self.decoder3 = nn.Linear(512, 768)\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Transformer model output\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        original_embeddings = transformer_output.last_hidden_state.max(dim=1).values\n",
    "        predictions = self.classifier(original_embeddings)\n",
    "        # Autoencoder forward pass\n",
    "        x = nn.functional.tanh(self.encoder1(original_embeddings))\n",
    "        x = nn.functional.tanh(self.encoder2(x))\n",
    "        x = nn.functional.tanh(self.encoder3(x))\n",
    "        x = nn.functional.tanh(self.decoder1(x))\n",
    "        x = nn.functional.tanh(self.decoder2(x))\n",
    "        reconstructed_embeddings = self.decoder3(x)\n",
    "        \n",
    "        return original_embeddings, reconstructed_embeddings, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1940dba-2be2-43ec-94bc-7680692d2b43",
   "metadata": {},
   "source": [
    "# Define Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dde5b309-f810-4989-ac81-67e7b4095907",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss_function = nn.MSELoss()\n",
    "ce_loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:13.263966793Z",
     "start_time": "2023-12-01T07:38:12.771780958Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5.00E-05)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "rec_loss_importance = 0.1\n",
    "factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:13.956536576Z",
     "start_time": "2023-12-01T07:38:13.948457670Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:19.550420285Z",
     "start_time": "2023-12-01T07:38:15.626978073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training skipped\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_name):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            original_embeddings, reconstructed_embeddings, predictions = model(input_ids, attention_mask)\n",
    "            rec_loss = rec_loss_function(original_embeddings, reconstructed_embeddings)\n",
    "            ce_loss = ce_loss_function(predictions, labels)\n",
    "            #print((1-rec_loss_importance) * ce_loss, rec_loss_importance * rec_loss * factor)\n",
    "            loss = (1-rec_loss_importance) * ce_loss + rec_loss_importance * rec_loss * factor\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss) \n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                original_embeddings, reconstructed_embeddings, predictions = model(input_ids, attention_mask)\n",
    "                rec_loss = rec_loss_function(original_embeddings, reconstructed_embeddings)\n",
    "                ce_loss = ce_loss_function(predictions, labels)\n",
    "                loss = (1-rec_loss_importance) * ce_loss + rec_loss_importance * rec_loss * factor\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")\n",
    "else:\n",
    "    print(\"training skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:27.299616993Z",
     "start_time": "2023-12-01T07:42:27.186388460Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:29.631025780Z",
     "start_time": "2023-12-01T07:42:28.516869555Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "val_embeddings = encode_sentences(fine_model, val_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_val_embeddings = encode_sentences(fine_model, oos_val_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25ad446a-9bf0-4850-93fa-56c99dbe35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e807196e-6ce7-4d39-b1c5-a476e509cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)\n",
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9123701602312398"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = val_scores + oos_val_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e871fc9-d12b-438d-b3b5-fed2c2eb483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142356006334116"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95afbbb6-1671-48af-aa2e-7b329d7c4210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9730705847438896"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e587bb47-b5ec-4d97-b0f4-f0de7f80d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            _, _, outputs = model(input_ids, attention_mask) \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())  # Move preds to CPU and convert to numpy\n",
    "            real_labels.extend(labels.detach().cpu().numpy())  # Move labels to CPU and convert to numpy\n",
    "    return predictions, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e108c51-acba-430f-94cd-1f02f2f76143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.9657105828587019\n"
     ]
    }
   ],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model = fine_model.to(device)\n",
    "preds, true_labels = get_predictions(fine_model, test_dataloader)\n",
    "\n",
    "# Now you can calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "print(f\"Classification Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d516ed52-b92c-43b4-bb06-6c702640d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41265c30-8bf8-498b-bb27-2027f9854ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_labels = np.unique(test_labels)\n",
    "# num_unique_labels = len(unique_labels)\n",
    " \n",
    "# ood_label = \"ood\"\n",
    " \n",
    "# # Combine embeddings\n",
    "# combined_embeddings = np.vstack((test_embeddings, oos_test_embeddings))\n",
    " \n",
    "# # Create labels for combined data\n",
    "# combined_labels = np.concatenate((test_labels, np.array([ood_label] * len(oos_test_embeddings))))\n",
    " \n",
    "# # TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# embeddings_2d = tsne.fit_transform(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67eeb689-ac1d-447f-bc88-e243febae44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# unique_labels = np.unique(combined_labels)\n",
    "# for label in unique_labels:\n",
    "#     indices = np.where(combined_labels == label)[0]\n",
    "#     if label == ood_label:\n",
    "#         # Specific color for OOD\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='black', s=0.5, alpha=0.7)\n",
    "#     else:\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='red', s=0.5, alpha=0.7)\n",
    "# plt.title('2D t-SNE Plot of Embeddings (Autoencoder)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d734ca-352b-4d53-9902-b1e821ab9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93aa8b-ea3b-4d79-964f-b5a3eebf161b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
