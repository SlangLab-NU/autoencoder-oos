{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897c26a-1cb2-4ad1-94d3-8cf70b70ee51",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3450eef-470e-4540-9c6a-99af76a5e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:04.721661140Z",
     "start_time": "2023-12-01T07:38:03.867605995Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6496a9a-49d4-4e85-8721-5cb1f0cf4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d938a-fceb-42f2-9002-e6c7500e75c4",
   "metadata": {},
   "source": [
    "# MTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186cfcd6-b370-448a-a3da-ecd526db0ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OOD Domains': ['timer'],\n",
       " 'Train Set Size': 14465,\n",
       " 'Validation Set Size': 2067,\n",
       " 'Test Set Size': 4134,\n",
       " 'OOS Validation Set Size': 491,\n",
       " 'OOS Test Set Size': 997}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def read_mtop_file(file_path):\n",
    "#     data = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         for line in file:\n",
    "#             fields = line.strip().split('\\t')\n",
    "#             if len(fields) < 8:\n",
    "#                 continue  # Skip any malformed lines\n",
    "#             record = {\n",
    "#                 'ID': fields[0],\n",
    "#                 'Intent': fields[1],\n",
    "#                 'Utterance': fields[3],\n",
    "#                 'Domain': fields[4]\n",
    "#             }\n",
    "#             data.append(record)\n",
    "#     return data\n",
    "\n",
    "# def select_ood_domains(domains, num_ood):\n",
    "#     return [\"timer\"]  # Hardcoded OOD domains\n",
    "\n",
    "# # Configuration parameters\n",
    "# english_dir = 'mtop/en'\n",
    "\n",
    "# # Read dataset\n",
    "# all_data = read_mtop_file(f'{english_dir}/test.txt') + \\\n",
    "#            read_mtop_file(f'{english_dir}/train.txt') + \\\n",
    "#            read_mtop_file(f'{english_dir}/eval.txt')\n",
    "\n",
    "# domains = set(record['Domain'] for record in all_data)\n",
    "# ood_domains = select_ood_domains(domains, 1)  # Using 1 as hardcoded number of OOD domains\n",
    "\n",
    "# # Separate OOD data based on domain\n",
    "# in_domain_data = [record for record in all_data if record['Domain'] not in ood_domains]\n",
    "# ood_data = [record for record in all_data if record['Domain'] in ood_domains]\n",
    "\n",
    "# intent_counts = Counter(record['Intent'] for record in in_domain_data)\n",
    "\n",
    "# # Filter intents with at least 10 instances\n",
    "# sufficient_data = [record for record in in_domain_data if intent_counts[record['Intent']] > 10]\n",
    "\n",
    "# train_val_data, test_data = train_test_split(\n",
    "#     sufficient_data, test_size=0.2, random_state=seed_value, stratify=[record['Intent'] for record in sufficient_data]\n",
    "# )\n",
    "# train_data, val_data = train_test_split(\n",
    "#     train_val_data, test_size=0.125, random_state=seed_value, stratify=[record['Intent'] for record in train_val_data]\n",
    "# )\n",
    "\n",
    "# # Split OOD data between validation and test\n",
    "# oos_val_data, oos_test_data = train_test_split(ood_data, test_size=0.67, random_state=seed_value)  # No stratification here since it's all OOD\n",
    "\n",
    "# # Extract sentences and labels\n",
    "# train_sentences = [record['Utterance'] for record in train_data]\n",
    "# train_labels = [record['Intent'] for record in train_data]\n",
    "\n",
    "# val_sentences = [record['Utterance'] for record in val_data]\n",
    "# val_labels = [record['Intent'] for record in val_data]\n",
    "\n",
    "# test_sentences = [record['Utterance'] for record in test_data]\n",
    "# test_labels = [record['Intent'] for record in test_data]\n",
    "\n",
    "# oos_val_sentences = [record['Utterance'] for record in oos_val_data]\n",
    "# oos_test_sentences = [record['Utterance'] for record in oos_test_data]\n",
    "\n",
    "# # Summary of splits\n",
    "# summary = {\n",
    "#     \"OOD Domains\": ood_domains,\n",
    "#     \"Train Set Size\": len(train_sentences),\n",
    "#     \"Validation Set Size\": len(val_sentences),\n",
    "#     \"Test Set Size\": len(test_sentences),\n",
    "#     \"OOS Validation Set Size\": len(oos_val_sentences),\n",
    "#     \"OOS Test Set Size\": len(oos_test_sentences)\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_name = \"ae_model_bert_mtop.pth\"\n",
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9e8e3-cc26-4d67-87a9-50ddc49a4fa3",
   "metadata": {},
   "source": [
    "# StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6b8204-027c-4956-85ed-c03f8f2c3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_number = 1  # Adjust this to the split number you're interested in\n",
    "\n",
    "# Define the base directory where the splits are stored\n",
    "base_dir = 'stackoverflow_data'  # Update this with your actual directory path\n",
    "\n",
    "# Construct the path to the specific split\n",
    "split_dir = os.path.join(base_dir, f'split{split_number}')\n",
    "\n",
    "# Function to load data from a .pkl file\n",
    "def load_data_from_split(split_dir, file_name):\n",
    "    with open(os.path.join(split_dir, file_name), 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Load the datasets\n",
    "train_sentences = load_data_from_split(split_dir, 'train_sentences.pkl')\n",
    "train_labels = load_data_from_split(split_dir, 'train_labels.pkl')\n",
    "val_sentences = load_data_from_split(split_dir, 'val_sentences.pkl')\n",
    "val_labels = load_data_from_split(split_dir, 'val_labels.pkl')\n",
    "test_sentences = load_data_from_split(split_dir, 'test_sentences.pkl')\n",
    "test_labels = load_data_from_split(split_dir, 'test_labels.pkl')\n",
    "oos_val_sentences = load_data_from_split(split_dir, 'oos_val_sentences.pkl')\n",
    "oos_test_sentences = load_data_from_split(split_dir, 'oos_test_sentences.pkl')\n",
    "model_name = \"ae_model_bert_Stack.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4172fcdc-58f7-49dd-8ffd-1b668d4e9d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14617"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oos_test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f96c05-0753-42a0-a801-beb4c2aed962",
   "metadata": {},
   "source": [
    "# CLINC150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46987c4-1aba-4b4f-a803-23052ca77f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:04.774005458Z",
     "start_time": "2023-12-01T07:38:04.723166259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"clinc150_uci/data_full.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "# Extracting data\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']\n",
    "\n",
    "oos_train_data = data['oos_train']\n",
    "oos_val_data = data['oos_val']\n",
    "oos_test_data = data['oos_test']\n",
    "\n",
    "# Get sentences and labels\n",
    "train_sentences = [item[0] for item in train_data]\n",
    "train_labels = [item[1] for item in train_data]\n",
    "\n",
    "val_sentences = [item[0] for item in val_data]\n",
    "val_labels = [item[1] for item in val_data]\n",
    "\n",
    "test_sentences = [item[0] for item in test_data]\n",
    "test_labels = [item[1] for item in test_data]\n",
    "\n",
    "oos_train_sentences = [item[0] for item in oos_train_data]\n",
    "oos_val_sentences = [item[0] for item in oos_val_data]\n",
    "oos_test_sentences = [item[0] for item in oos_test_data]\n",
    "model_name = \"ae_model_bert_CLINC150.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ba83dc-95ed-4e5a-9484-ce28204ec991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{seed_value}_{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12dd80-1971-4215-8486-8de1d655d5ed",
   "metadata": {},
   "source": [
    "# Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f9dd99-07c9-4e5a-993b-e73dfd0be95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:05.423215976Z",
     "start_time": "2023-12-01T07:38:05.406651396Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Fit the label encoder and transform labels to integers\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_val_labels = label_encoder.fit_transform(val_labels)\n",
    "encoded_test_labels = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c1c26-f71a-4395-8ef9-01b52c930fdc",
   "metadata": {},
   "source": [
    "# Tokenize our sentences and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3db18c-5804-4f09-9f6b-1f3131415fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:07.327824427Z",
     "start_time": "2023-12-01T07:38:06.135883081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztybigcat/Desktop/my_own_ood/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab67fd0-7073-4495-b464-8ecb4b2b3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length for tokenizer: 58\n"
     ]
    }
   ],
   "source": [
    "tokenized_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in train_sentences]\n",
    "max_length = max(tokenized_lengths)\n",
    "print(f\"Max length for tokenizer: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9090dc4-fe28-4c23-b21b-5ba229eb8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(sentences, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 2. Create the dataset\n",
    "train_dataset = TextDataset(train_sentences, encoded_train_labels, tokenizer, max_length)\n",
    "val_dataset = TextDataset(val_sentences, encoded_val_labels, tokenizer, max_length)\n",
    "test_dataset = TextDataset(test_sentences, encoded_test_labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d137706-db61-4152-80e7-8068e75e381f",
   "metadata": {},
   "source": [
    "# Define functions to encode our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a8213f6-4be9-41e6-95cd-07d9fa53a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2952162c-8da1-4cae-ae22-5eb06fa41ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:07.931914441Z",
     "start_time": "2023-12-01T07:38:07.768420485Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer_model.eval()\n",
    "transformer_model = transformer_model.to(device)\n",
    "def encode_sentences(model, sentences, tokenizer=tokenizer, batch_size=256):\n",
    "    model = model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Move the batch to the same device as the model\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
    "        sentence_embeddings.append(pooled_output)\n",
    "\n",
    "    # Concatenate all batched embeddings and move to CPU in one go\n",
    "    sentence_embeddings_np = torch.cat(sentence_embeddings, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_embeddings_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de918d-3a50-4ea9-84d2-8d2d03a0fa4e",
   "metadata": {},
   "source": [
    "# Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c586c9a1-9e7c-4fbe-9ff8-087d8bd5f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:09.600227529Z",
     "start_time": "2023-12-01T07:38:09.154881377Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder1 = nn.Linear(768, 512)\n",
    "        self.encoder2 = nn.Linear(512, 64)\n",
    "        self.encoder3 = nn.Linear(64, 16)\n",
    "        # Decoder layers\n",
    "        self.decoder1 = nn.Linear(16, 64)\n",
    "        self.decoder2 = nn.Linear(64, 512)\n",
    "        self.decoder3 = nn.Linear(512, 768)\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Transformer model output\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        original_embeddings = transformer_output.last_hidden_state.max(dim=1).values\n",
    "        predictions = self.classifier(original_embeddings)\n",
    "        # Autoencoder forward pass\n",
    "        x = nn.functional.tanh(self.encoder1(original_embeddings))\n",
    "        x = nn.functional.tanh(self.encoder2(x))\n",
    "        x = nn.functional.tanh(self.encoder3(x))\n",
    "        x = nn.functional.tanh(self.decoder1(x))\n",
    "        x = nn.functional.tanh(self.decoder2(x))\n",
    "        reconstructed_embeddings = self.decoder3(x)\n",
    "        \n",
    "        return original_embeddings, reconstructed_embeddings, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1940dba-2be2-43ec-94bc-7680692d2b43",
   "metadata": {},
   "source": [
    "# Define Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde5b309-f810-4989-ac81-67e7b4095907",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss_function = nn.MSELoss()\n",
    "ce_loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98181737-c127-41a7-beb3-ab66172b9303",
   "metadata": {},
   "source": [
    "# Initiallize everything else we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9836c07-b6b3-43e0-adf9-178ebcdcea68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:13.263966793Z",
     "start_time": "2023-12-01T07:38:12.771780958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztybigcat/Desktop/my_own_ood/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "unique_intents = list(set(train_labels)) \n",
    "transformer_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "transformer_model.to(device)\n",
    "model = TextClassifier(transformer_model, len(unique_intents))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5.00E-05)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "batch_size= 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "rec_loss_importance = 0.1\n",
    "factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed9508c7-081f-469a-96d2-16461816e58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:38:13.956536576Z",
     "start_time": "2023-12-01T07:38:13.948457670Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9a636-af8e-4d4c-abb2-fc1370da38a3",
   "metadata": {},
   "source": [
    "# Training Loop!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "903bf045-515e-4c93-999f-136d077d391c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:19.550420285Z",
     "start_time": "2023-12-01T07:38:15.626978073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training skipped\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_name):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            original_embeddings, reconstructed_embeddings, predictions = model(input_ids, attention_mask)\n",
    "            rec_loss = rec_loss_function(original_embeddings, reconstructed_embeddings)\n",
    "            ce_loss = ce_loss_function(predictions, labels)\n",
    "            #print((1-rec_loss_importance) * ce_loss, rec_loss_importance * rec_loss * factor)\n",
    "            loss = (1-rec_loss_importance) * ce_loss + rec_loss_importance * rec_loss * factor\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "    \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_train_loss) \n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "                original_embeddings, reconstructed_embeddings, predictions = model(input_ids, attention_mask)\n",
    "                rec_loss = rec_loss_function(original_embeddings, reconstructed_embeddings)\n",
    "                ce_loss = ce_loss_function(predictions, labels)\n",
    "                loss = (1-rec_loss_importance) * ce_loss + rec_loss_importance * rec_loss * factor\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the model\n",
    "            torch.save(model, model_name)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Lower validation loss found. Model saved.\")\n",
    "        validation_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.5e}, Validation Loss: {avg_val_loss:.5e}\")\n",
    "else:\n",
    "    print(\"training skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32293-8028-4ebe-8a10-3fc012782ad1",
   "metadata": {},
   "source": [
    "# Calculate means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eba64b40-f47d-4a2b-9fe6-3839dc18890e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:27.299616993Z",
     "start_time": "2023-12-01T07:42:27.186388460Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model.eval()  # Put the model in evaluation mode\n",
    "fine_model = fine_model.to(device)\n",
    "fine_model = fine_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d313cc44-0e2c-4851-88c1-147dd23ac09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:42:29.631025780Z",
     "start_time": "2023-12-01T07:42:28.516869555Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = encode_sentences(fine_model, train_sentences)\n",
    "val_embeddings = encode_sentences(fine_model, val_sentences)\n",
    "test_embeddings = encode_sentences(fine_model, test_sentences)\n",
    "oos_val_embeddings = encode_sentences(fine_model, oos_val_sentences)\n",
    "oos_test_embeddings = encode_sentences(fine_model, oos_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ad446a-9bf0-4850-93fa-56c99dbe35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_means = {}\n",
    "for encoded_label in np.unique(encoded_train_labels):\n",
    "    # Find indices where the encoded label matches\n",
    "    indices = np.where(encoded_train_labels == encoded_label)[0]\n",
    "    \n",
    "    # Calculate the mean embedding for the current intent\n",
    "    intent_embeddings = train_embeddings[indices]\n",
    "    intent_mean = np.mean(intent_embeddings, axis=0)\n",
    "    \n",
    "    # Use the encoded label as the dictionary key\n",
    "    intent_means[encoded_label] = intent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73181d6e-667b-4cbe-a3f3-ce68aa139d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('autoencoder_embeddings.npz', \n",
    "         intent_means=np.array(list(intent_means.values())), \n",
    "         oos_test_embeddings=oos_test_embeddings, \n",
    "         intent_labels=np.array(list(intent_means.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e807196e-6ce7-4d39-b1c5-a476e509cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(train_embeddings, rowvar=False)\n",
    "cov_inverse = inv(covariance)\n",
    "def min_mahalanobis_for_sample(sample, intent_means, cov_inverse):\n",
    "    distances = [distance.mahalanobis(sample, mean, cov_inverse) for mean in intent_means.values()]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d69512f-d0fb-4231-9f7e-47f2c32fac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.025941410649846"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = np.trace(covariance)\n",
    "total_variance = np.sqrt(trace)\n",
    "total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07795d99-5725-4fed-a1d0-b1bb9dc6af54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752825923710592"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in val_embeddings]\n",
    "oos_val_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_val_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(val_scores) + [1] * len(oos_val_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = val_scores + oos_val_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e871fc9-d12b-438d-b3b5-fed2c2eb483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188398058464923"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute minimum Mahalanobis distances for samples in test_embeddings and oos_test_embeddings\n",
    "test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in test_embeddings]\n",
    "oos_test_scores = [min_mahalanobis_for_sample(sample, intent_means, cov_inverse) for sample in oos_test_embeddings]\n",
    "\n",
    "# True binary labels: 0 for in-domain and 1 for OOD\n",
    "y_true = [0] * len(test_scores) + [1] * len(oos_test_scores)\n",
    "\n",
    "# Combine the scores\n",
    "y_scores = test_scores + oos_test_scores\n",
    "\n",
    "# Compute AUPR\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "aupr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95afbbb6-1671-48af-aa2e-7b329d7c4210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782435555555555"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = roc_auc_score(y_true, y_scores)\n",
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587bb47-b5ec-4d97-b0f4-f0de7f80d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            _, _, outputs = model(input_ids, attention_mask) \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())  # Move preds to CPU and convert to numpy\n",
    "            real_labels.extend(labels.detach().cpu().numpy())  # Move labels to CPU and convert to numpy\n",
    "    return predictions, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e108c51-acba-430f-94cd-1f02f2f76143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_model = torch.load(model_name)\n",
    "fine_model = fine_model.to(device)\n",
    "preds, true_labels = get_predictions(fine_model, test_dataloader)\n",
    "\n",
    "# Now you can calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "print(f\"Classification Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516ed52-b92c-43b4-bb06-6c702640d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the figure and axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot the histograms\n",
    "# plt.hist(test_scores, bins=50, alpha=0.5, label='In-domain')\n",
    "# plt.hist(oos_test_scores, bins=50, alpha=0.5, label='Out-of-domain')\n",
    "\n",
    "# # Add legend, title, and labels\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Distribution of Minimum Mahalanobis Distances')\n",
    "# plt.xlabel('Mahalanobis Distance')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41265c30-8bf8-498b-bb27-2027f9854ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_labels = np.unique(test_labels)\n",
    "# num_unique_labels = len(unique_labels)\n",
    " \n",
    "# ood_label = \"ood\"\n",
    " \n",
    "# # Combine embeddings\n",
    "# combined_embeddings = np.vstack((test_embeddings, oos_test_embeddings))\n",
    " \n",
    "# # Create labels for combined data\n",
    "# combined_labels = np.concatenate((test_labels, np.array([ood_label] * len(oos_test_embeddings))))\n",
    " \n",
    "# # TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# embeddings_2d = tsne.fit_transform(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eeb689-ac1d-447f-bc88-e243febae44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# unique_labels = np.unique(combined_labels)\n",
    "# for label in unique_labels:\n",
    "#     indices = np.where(combined_labels == label)[0]\n",
    "#     if label == ood_label:\n",
    "#         # Specific color for OOD\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='black', s=0.5, alpha=0.7)\n",
    "#     else:\n",
    "#         plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=label, color='red', s=0.5, alpha=0.7)\n",
    "# plt.title('2D t-SNE Plot of Embeddings (Autoencoder)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d734ca-352b-4d53-9902-b1e821ab9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93aa8b-ea3b-4d79-964f-b5a3eebf161b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
